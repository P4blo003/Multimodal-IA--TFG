2025/06/23 13:55:03 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T13:55:03.458Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T13:55:03.459Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T13:55:03.460Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T13:55:03.460Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T13:55:03.610Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 14:15:30 | 200 |     373.169µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:15:30 | 200 |     262.738µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:15:30.452Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:15:30.563Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:15:30.579Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:15:30.581Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T14:15:30.665Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="118.4 GiB" free_swap="8.0 GiB"
time=2025-06-23T14:15:30.665Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T14:15:30.701Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:15:30.701Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:15:30.703Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:15:30.704Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:15:30.704Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:15:30.704Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:15:30.704Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:15:30.704Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 46695"
time=2025-06-23T14:15:30.704Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T14:15:30.704Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T14:15:30.704Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T14:15:30.711Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T14:15:30.711Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:46695"
time=2025-06-23T14:15:30.757Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:15:30.758Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T14:15:30.758Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T14:15:30.758Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T14:15:30.795Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T14:15:30.852Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T14:15:30.852Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T14:15:30.956Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T14:15:31.688Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:15:31.691Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:15:31.691Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:15:31.691Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:15:31.691Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:15:31.691Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:15:31.712Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T14:15:31.712Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T14:15:31.961Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 14:15:31 | 200 |   1.52703282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:18:44 | 200 |      346.07µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:18:44 | 200 |      246.56µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:18:44.378Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:18:44 | 200 |   18.256906ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:22:19 | 200 |     373.541µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:22:19 | 200 |     257.783µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:22:19.322Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:22:19 | 200 |    18.86459ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:27:13 | 200 |     370.012µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:27:13 | 200 |     249.905µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:27:13.805Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:27:13 | 200 |   18.443175ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:29:44 | 200 |     343.125µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:29:44 | 200 |     292.083µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:29:44.843Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:29:44 | 200 |   18.420939ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:30:23 | 200 |     397.018µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:30:23 | 200 |     292.283µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:30:23.481Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:30:23 | 200 |   18.609665ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 14:31:15 | 200 |     349.005µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:31:15 | 200 |     261.676µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:31:15.143Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 14:31:15 | 200 |   18.734299ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T14:36:20.273Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.100013858 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T14:36:20.523Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.349781311 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T14:36:20.774Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.600396135 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
[GIN] 2025/06/23 - 14:48:37 | 200 |    1.184969ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:48:37 | 200 |     1.09079ms |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:48:37.521Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:48:37.638Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:48:37.654Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:48:37.656Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15647047680 required="5.6 GiB"
time=2025-06-23T14:48:37.748Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="117.4 GiB" free_swap="8.0 GiB"
time=2025-06-23T14:48:37.749Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[14.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T14:48:37.782Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:48:37.783Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:48:37.785Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:48:37.785Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:48:37.786Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:48:37.786Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:48:37.786Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:48:37.786Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 33959"
time=2025-06-23T14:48:37.786Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T14:48:37.786Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T14:48:37.786Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T14:48:37.796Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T14:48:37.796Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:33959"
time=2025-06-23T14:48:37.843Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:48:37.843Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T14:48:37.843Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T14:48:37.843Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T14:48:37.881Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T14:48:37.939Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T14:48:37.939Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T14:48:38.038Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T14:48:38.804Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:48:38.806Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:48:38.806Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:48:38.806Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:48:38.806Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:48:38.806Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:48:38.826Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T14:48:38.826Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T14:48:39.043Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 14:48:39 | 200 |  1.554102335s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T14:53:44.143Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.098588964 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T14:53:44.394Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.349427245 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T14:53:44.643Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.598414794 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
[GIN] 2025/06/23 - 14:54:29 | 200 |     329.196µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:54:29 | 200 |     241.587µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:54:29.350Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:54:29.460Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:54:29.477Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:54:29.479Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T14:54:29.564Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="117.7 GiB" free_swap="8.0 GiB"
time=2025-06-23T14:54:29.565Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T14:54:29.596Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:54:29.597Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:54:29.599Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:54:29.599Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:54:29.599Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:54:29.599Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:54:29.599Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:54:29.599Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 33799"
time=2025-06-23T14:54:29.599Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T14:54:29.599Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T14:54:29.600Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T14:54:29.607Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T14:54:29.607Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:33799"
time=2025-06-23T14:54:29.665Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:54:29.666Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T14:54:29.666Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T14:54:29.666Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T14:54:29.706Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T14:54:29.770Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T14:54:29.770Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T14:54:29.852Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T14:54:30.628Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:54:30.630Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:54:30.630Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:54:30.630Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:54:30.630Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:54:30.630Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:54:30.650Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T14:54:30.650Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T14:54:30.857Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 14:54:30 | 200 |  1.525165264s |       127.0.0.1 | POST     "/api/chat"
2025/06/23 14:57:18 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T14:57:18.780Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T14:57:18.780Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T14:57:18.780Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T14:57:18.780Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T14:57:18.936Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 14:57:28 | 200 |     385.428µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 14:57:28 | 200 |      268.08µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T14:57:28.693Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:57:28.804Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:57:28.822Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:57:28.823Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T14:57:28.909Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="117.7 GiB" free_swap="8.0 GiB"
time=2025-06-23T14:57:28.910Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T14:57:28.943Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:57:28.943Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:57:28.945Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:57:28.945Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:57:28.945Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:57:28.945Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:57:28.945Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:57:28.945Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 36251"
time=2025-06-23T14:57:28.946Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T14:57:28.946Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T14:57:28.946Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T14:57:28.956Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T14:57:28.956Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:36251"
time=2025-06-23T14:57:29.005Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T14:57:29.006Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T14:57:29.006Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T14:57:29.006Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T14:57:29.043Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T14:57:29.098Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T14:57:29.098Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T14:57:29.198Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T14:57:29.947Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T14:57:29.949Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T14:57:29.949Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T14:57:29.949Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T14:57:29.949Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T14:57:29.949Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T14:57:29.972Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T14:57:29.972Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T14:57:30.203Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 14:57:30 | 200 |  1.529195085s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:02:35.302Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.097759715 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T15:02:35.552Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.348044785 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T15:02:35.802Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.598224574 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
[GIN] 2025/06/23 - 15:02:52 | 200 |     348.765µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:02:52 | 200 |     255.075µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:02:52.779Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T15:02:52.891Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T15:02:52.909Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T15:02:52.911Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T15:02:52.995Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="117.8 GiB" free_swap="8.0 GiB"
time=2025-06-23T15:02:52.996Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T15:02:53.031Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T15:02:53.031Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T15:02:53.034Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T15:02:53.034Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T15:02:53.034Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T15:02:53.034Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T15:02:53.034Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T15:02:53.034Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 45981"
time=2025-06-23T15:02:53.034Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T15:02:53.034Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T15:02:53.034Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T15:02:53.046Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T15:02:53.047Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:45981"
time=2025-06-23T15:02:53.099Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T15:02:53.099Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T15:02:53.099Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T15:02:53.099Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T15:02:53.137Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T15:02:53.194Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T15:02:53.194Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T15:02:53.286Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T15:02:54.066Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T15:02:54.068Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T15:02:54.068Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T15:02:54.068Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T15:02:54.068Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T15:02:54.068Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T15:02:54.089Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T15:02:54.089Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T15:02:54.290Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 15:02:54 | 200 |  1.529172241s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:05:35 | 200 |     454.919µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:05:35 | 200 |     243.451µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:05:35.113Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:05:35 | 200 |   18.553212ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:05:42.429Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:05:43 | 200 |  795.322772ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 15:06:09 | 200 |     331.358µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:06:09 | 200 |     269.041µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:06:09.127Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:06:09 | 200 |   19.050003ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:06:20.544Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:06:21 | 200 |  1.418085663s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 15:10:15 | 200 |     412.845µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:10:15 | 200 |     247.243µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:10:15.344Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:10:15 | 200 |   18.417248ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:10:56 | 200 |     329.817µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:10:56 | 200 |     243.364µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:10:56.088Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:10:56 | 200 |   19.169329ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:11:43 | 200 |     352.247µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:11:43 | 200 |     285.943µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:11:43.937Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:11:43 | 200 |    18.81339ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:12:41 | 200 |     366.611µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:12:41 | 200 |     275.256µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:12:41.715Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:12:41 | 200 |   18.427634ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:13:35 | 200 |     348.861µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:13:35 | 200 |     249.304µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:13:35.421Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:13:35 | 200 |   18.669029ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:14:17 | 200 |     325.252µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:14:17 | 200 |     271.105µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:14:17.020Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:14:17 | 200 |   18.399269ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:16:26 | 200 |     337.747µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:16:26 | 200 |     259.669µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:16:26.206Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:16:26 | 200 |   25.588511ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:16:32.598Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:16:33 | 200 |  905.674019ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 15:17:30 | 200 |     922.916µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:17:30 | 200 |     414.483µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:17:30.289Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:17:30 | 200 |    33.20752ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/23 - 15:18:04 | 200 |     345.338µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:18:04 | 200 |     235.944µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:18:05.016Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:18:05 | 200 |   18.014242ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:18:08.404Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:18:09 | 200 |  1.177666407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 15:18:50 | 200 |     332.624µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 15:18:50 | 200 |     258.822µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T15:18:50.753Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:18:50 | 200 |   18.911822ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T15:18:55.092Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:18:56 | 200 |  1.117848939s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T15:19:13.100Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 15:19:14 | 200 |  927.307015ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T15:24:19.109Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.098702262 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T15:24:19.360Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.349530164 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T15:24:19.610Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.599576862 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
2025/06/23 16:07:32 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T16:07:32.330Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T16:07:32.331Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T16:07:32.331Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T16:07:32.331Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T16:07:32.483Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 16:07:43 | 200 |     378.704µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 16:07:43 | 200 |      341.52µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T16:07:43.932Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:07:44.049Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:07:44.070Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:07:44.071Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T16:07:44.158Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="121.1 GiB" free_swap="8.0 GiB"
time=2025-06-23T16:07:44.158Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T16:07:44.196Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:07:44.196Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:07:44.198Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:07:44.198Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:07:44.198Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:07:44.198Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:07:44.198Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:07:44.198Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 44443"
time=2025-06-23T16:07:44.199Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T16:07:44.199Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T16:07:44.199Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T16:07:44.209Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T16:07:44.210Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:44443"
time=2025-06-23T16:07:44.253Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:07:44.253Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T16:07:44.253Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T16:07:44.253Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T16:07:44.291Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T16:07:44.346Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T16:07:44.346Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T16:07:44.450Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T16:07:45.207Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:07:45.210Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:07:45.210Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:07:45.210Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:07:45.210Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:07:45.210Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:07:45.230Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T16:07:45.230Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T16:07:45.456Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 16:07:45 | 200 |  1.561108089s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T16:07:58.078Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 16:07:59 | 200 |  1.157617577s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 16:08:45 | 200 |     348.755µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 16:08:45 | 200 |     245.671µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T16:08:45.219Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 16:08:45 | 200 |   18.545972ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T16:08:52.096Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 16:08:53 | 200 |  1.055741501s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T16:13:58.228Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.097316201 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T16:13:58.478Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.346881909 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-06-23T16:13:58.728Z level=WARN source=sched.go:655 msg="gpu VRAM usage didn't recover within timeout" seconds=5.597217973 model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
[GIN] 2025/06/23 - 16:20:20 | 200 |     307.867µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 16:20:20 | 200 |     234.798µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T16:20:20.696Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:20:20.805Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:20:20.822Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:20:20.823Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T16:20:20.907Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.5 GiB" free_swap="8.0 GiB"
time=2025-06-23T16:20:20.907Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T16:20:20.940Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:20:20.941Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:20:20.943Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:20:20.943Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:20:20.943Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:20:20.943Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:20:20.943Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:20:20.943Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 36555"
time=2025-06-23T16:20:20.943Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T16:20:20.943Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T16:20:20.944Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T16:20:20.954Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T16:20:20.954Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:36555"
time=2025-06-23T16:20:21.003Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:20:21.003Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T16:20:21.003Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T16:20:21.003Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T16:20:21.042Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T16:20:21.099Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T16:20:21.099Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T16:20:21.195Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T16:20:21.937Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:20:21.939Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:20:21.939Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:20:21.939Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:20:21.939Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:20:21.939Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:20:21.958Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T16:20:21.958Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T16:20:22.199Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/23 - 16:20:22 | 200 |  1.521734543s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T16:20:26.132Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 16:20:27 | 200 |  1.626932414s |       127.0.0.1 | POST     "/api/generate"
2025/06/23 16:30:41 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T16:30:41.247Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T16:30:41.247Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T16:30:41.248Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T16:30:41.248Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T16:30:41.361Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="14.6 GiB"
[GIN] 2025/06/23 - 16:30:51 | 200 |     394.182µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 16:30:51 | 200 |     274.834µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T16:30:52.004Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:30:52.116Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:30:52.133Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:30:52.134Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T16:30:52.220Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-23T16:30:52.221Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T16:30:52.257Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:30:52.258Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:30:52.260Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:30:52.260Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:30:52.260Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:30:52.260Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:30:52.260Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:30:52.260Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 45379"
time=2025-06-23T16:30:52.261Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T16:30:52.261Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T16:30:52.261Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T16:30:52.271Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T16:30:52.271Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:45379"
time=2025-06-23T16:30:52.321Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:30:52.321Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T16:30:52.321Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T16:30:52.321Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T16:30:52.359Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T16:30:52.415Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T16:30:52.415Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T16:30:52.513Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T16:30:53.197Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:30:53.199Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:30:53.199Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:30:53.199Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:30:53.199Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:30:53.199Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:30:53.219Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T16:30:53.219Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T16:30:53.266Z level=INFO source=server.go:628 msg="llama runner started in 1.01 seconds"
[GIN] 2025/06/23 - 16:30:53 | 200 |  1.280021045s |       127.0.0.1 | POST     "/api/chat"
2025/06/23 16:32:17 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T16:32:17.971Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T16:32:17.971Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T16:32:17.971Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T16:32:17.971Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T16:32:18.129Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 16:32:24 | 200 |     367.068µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 16:32:24 | 200 |     260.373µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T16:32:24.040Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:32:24.151Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:32:24.178Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:32:24.179Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-23T16:32:24.266Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.6 GiB" free_swap="8.0 GiB"
time=2025-06-23T16:32:24.267Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-23T16:32:24.299Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:32:24.300Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:32:24.302Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:32:24.302Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:32:24.302Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:32:24.302Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:32:24.302Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:32:24.302Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 45733"
time=2025-06-23T16:32:24.303Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T16:32:24.303Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T16:32:24.303Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T16:32:24.309Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-23T16:32:24.310Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:45733"
time=2025-06-23T16:32:24.352Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T16:32:24.353Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-23T16:32:24.353Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-23T16:32:24.353Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T16:32:24.391Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T16:32:24.448Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-23T16:32:24.448Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-23T16:32:24.555Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-23T16:32:25.257Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-23T16:32:25.259Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-23T16:32:25.259Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-23T16:32:25.259Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-23T16:32:25.259Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-23T16:32:25.259Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-23T16:32:25.278Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-23T16:32:25.278Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-23T16:32:25.308Z level=INFO source=server.go:628 msg="llama runner started in 1.01 seconds"
[GIN] 2025/06/23 - 16:32:25 | 200 |  1.296280595s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T16:32:38.182Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 16:32:38 | 200 |  818.085078ms |       127.0.0.1 | POST     "/api/generate"
2025/06/23 17:42:15 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T17:42:15.886Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T17:42:15.886Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T17:42:15.886Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T17:42:15.886Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T17:42:16.038Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 17:42:22 | 200 |     383.454µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 17:42:22 | 200 |     256.971µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T17:42:22.987Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:42:23.088Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:42:23.095Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:42:23.096Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T17:42:23.096Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="3.7 GiB"
time=2025-06-23T17:42:23.181Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.3 GiB" free_swap="8.0 GiB"
time=2025-06-23T17:42:23.182Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T17:42:23.182Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-23T17:42:23.320Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 38261"
time=2025-06-23T17:42:23.321Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T17:42:23.321Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T17:42:23.321Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T17:42:23.327Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T17:42:23.364Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T17:42:23.364Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:38261"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-23T17:42:23.572Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-23T17:42:25.329Z level=INFO source=server.go:628 msg="llama runner started in 2.01 seconds"
[GIN] 2025/06/23 - 17:42:25 | 200 |  2.410446703s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T17:45:08.166Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:45:11 | 200 |  3.198810742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/23 - 17:45:26 | 200 |     331.732µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 17:45:26 | 200 |     261.027µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T17:45:26.454Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:45:26 | 200 |    8.206614ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T17:46:00.952Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:04 | 200 |  3.100317687s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:04.061Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:06 | 200 |  2.392415158s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:06.462Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:08 | 200 |  1.682890064s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:08.153Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:10 | 200 |  2.118213662s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:10.281Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:13 | 200 |  3.402043424s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:13.691Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:14 | 200 |  462.408756ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:14.162Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:14 | 200 |   824.56247ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:14.996Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:15 | 200 |   475.08394ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:15.480Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:17 | 200 |  1.649664103s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:17.138Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:18 | 200 |  1.859815859s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:19.007Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:21 | 200 |  2.419972183s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:21.446Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:22 | 200 |  1.076488783s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:22.530Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:23 | 200 |   864.28241ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:23.409Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:23 | 200 |   355.79414ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:23.773Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:26 | 200 |  2.682537543s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:26.469Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:26 | 200 |  100.621725ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:26.575Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:26 | 200 |  224.622131ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:26.808Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:29 | 200 |  2.855597847s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:29.678Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:30 | 200 |  1.080159712s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:30.762Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:34 | 200 |  3.812189707s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:34.583Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:34 | 200 |  144.575763ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:34.737Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:34 | 200 |  133.568221ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:34.879Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:35 | 200 |  983.593736ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:35.871Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:38 | 200 |   2.91973876s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:38.805Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:41 | 200 |  3.162078036s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:41.971Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:42 | 200 |  351.915862ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:42.332Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:43 | 200 |  1.276464592s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:43.618Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:46 | 200 |  3.333242261s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:46.960Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:48 | 200 |  2.044718662s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:49.013Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:49 | 200 |  973.547809ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:49.995Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:51 | 200 |  1.251279662s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:51.256Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:53 | 200 |  2.703723464s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:53.976Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:46:58 | 200 |  4.616425574s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:46:58.596Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:47:04 | 200 |  5.718952324s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:47:04.334Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:47:06 | 200 |  2.596774089s |       127.0.0.1 | POST     "/api/generate"
2025/06/23 17:54:22 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T17:54:22.385Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T17:54:22.386Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T17:54:22.386Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T17:54:22.386Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T17:54:22.537Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 17:54:29 | 200 |     375.318µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 17:54:29 | 200 |     279.554µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T17:54:29.761Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:54:29.863Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:54:29.870Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T17:54:29.870Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T17:54:29.870Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-23T17:54:29.870Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-23T17:54:29.870Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="6.5 GiB"
time=2025-06-23T17:54:29.954Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.3 GiB" free_swap="8.0 GiB"
time=2025-06-23T17:54:29.954Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T17:54:29.954Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-23T17:54:29.954Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-23T17:54:29.954Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-23T17:54:30.091Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 2 --port 46377"
time=2025-06-23T17:54:30.091Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T17:54:30.091Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T17:54:30.091Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T17:54:30.098Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T17:54:30.135Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T17:54:30.135Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:46377"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-23T17:54:30.343Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.01 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
init:      CUDA0 KV buffer size =  1024.00 MiB
llama_context: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 2
time=2025-06-23T17:54:30.844Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/23 - 17:54:30 | 200 |  1.097039385s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T17:54:37.901Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:41 | 200 |  3.488805995s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:41.400Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:43 | 200 |  2.575388864s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:43.985Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:44 | 200 |  1.002984542s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:44.995Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:45 | 200 |   588.92252ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:45.592Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:47 | 200 |   1.60066053s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:47.202Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:47 | 200 |   82.640418ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:47.294Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:47 | 200 |  676.928726ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:47.979Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:48 | 200 |   279.61667ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:48.267Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:48 | 200 |  570.811864ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:48.847Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:51 | 200 |  2.406603454s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:51.262Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:52 | 200 |  1.722444553s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:53.001Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:53 | 200 |  148.826216ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:53.156Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:54 | 200 |  1.053061616s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:54.217Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:54 | 200 |   336.49709ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:54.559Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:56 | 200 |  2.397682013s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:56.966Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:57 | 200 |  159.898173ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:57.134Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:57 | 200 |    254.0318ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:57.397Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:54:57 | 200 |   410.32667ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:54:57.816Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:00 | 200 |  2.558388585s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:00.383Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:04 | 200 |  3.833718169s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:04.226Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:04 | 200 |  435.284138ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:04.671Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:04 | 200 |  220.621872ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:04.899Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:05 | 200 |   279.21871ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:05.187Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:09 | 200 |  4.053421676s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:09.250Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:12 | 200 |  3.203138032s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:12.461Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:12 | 200 |   99.606978ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:12.570Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:13 | 200 |  630.194524ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:13.209Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:16 | 200 |  2.861946851s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:16.080Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:20 | 200 |  4.249837667s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:20.338Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:21 | 200 |  792.291114ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:21.139Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:23 | 200 |   2.25435942s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:23.402Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:26 | 200 |  3.191837003s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:26.606Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:31 | 200 |  5.019999331s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:31.635Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:38 | 200 |  7.230390562s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T17:55:38.874Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 17:55:41 | 200 |  2.166462139s |       127.0.0.1 | POST     "/api/generate"
2025/06/23 18:03:10 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T18:03:10.300Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T18:03:10.301Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T18:03:10.301Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T18:03:10.302Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T18:03:10.468Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 18:03:15 | 200 |     373.997µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 18:03:15 | 200 |     266.479µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T18:03:15.936Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:03:16.036Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:03:16.043Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:03:16.043Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T18:03:16.043Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-23T18:03:16.043Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-23T18:03:16.043Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="6.2 GiB"
time=2025-06-23T18:03:16.134Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.3 GiB" free_swap="8.0 GiB"
time=2025-06-23T18:03:16.134Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-23T18:03:16.134Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-23T18:03:16.134Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-23T18:03:16.134Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-23T18:03:16.268Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 2 --port 45601"
time=2025-06-23T18:03:16.269Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T18:03:16.269Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T18:03:16.269Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T18:03:16.278Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T18:03:16.321Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T18:03:16.322Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:45601"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: special tokens cache size = 256
time=2025-06-23T18:03:16.520Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.01 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
init:      CUDA0 KV buffer size =  1024.00 MiB
llama_context: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 2
time=2025-06-23T18:03:17.022Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/23 - 18:03:17 | 200 |  1.100201459s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T18:03:28.520Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:32 | 200 |    4.2043302s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:32.765Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:37 | 200 |  5.127149001s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:37.907Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:42 | 200 |  4.366654937s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:42.275Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:46 | 200 |  4.019305223s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:46.303Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:52 | 200 |  6.228044838s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:52.540Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:52 | 200 |  146.647469ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:52.695Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:53 | 200 |  1.042618153s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:53.747Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:54 | 200 |  527.696861ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:54.283Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:56 | 200 |   2.06105412s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:56.353Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:03:59 | 200 |  3.463808614s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:03:59.826Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:03 | 200 |  3.813518772s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:03.662Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:04 | 200 |  519.852117ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:04.186Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:05 | 200 |   1.49270163s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:05.695Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:06 | 200 |  487.171599ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:06.184Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:12 | 200 |  5.875179659s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:12.068Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:12 | 200 |  543.859142ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:12.621Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:13 | 200 |  387.182619ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:13.016Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:16 | 200 |  3.716139276s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:16.742Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:19 | 200 |  3.043824664s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:19.794Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:22 | 200 |  2.839080724s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:22.642Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:26 | 200 |  3.524996287s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:26.176Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:27 | 200 |  1.625785783s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:27.811Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:29 | 200 |  1.532402769s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:29.352Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:32 | 200 |  3.577880688s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:32.938Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:37 | 200 |  4.545081631s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:37.493Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:38 | 200 |  1.182967436s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:38.684Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:39 | 200 |  1.289331754s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:39.983Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:46 | 200 |  6.788709314s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:46.784Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:51 | 200 |  4.761517966s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:51.550Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:52 | 200 |  618.025724ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:52.177Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:53 | 200 |  1.755097136s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:53.941Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:04:58 | 200 |  4.503612699s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:04:58.465Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:05:01 | 200 |  2.834731478s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:05:01.300Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:05:09 | 200 |  7.807469912s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:05:09.124Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:05:13 | 200 |  3.946765332s |       127.0.0.1 | POST     "/api/generate"
2025/06/23 18:11:03 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-23T18:11:03.737Z level=INFO source=images.go:463 msg="total blobs: 27"
time=2025-06-23T18:11:03.737Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-23T18:11:03.738Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-23T18:11:03.738Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-23T18:11:03.895Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/23 - 18:11:09 | 200 |     382.213µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/23 - 18:11:09 | 200 |      465.28µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-23T18:11:09.954Z level=INFO source=download.go:177 msg="downloading aabd4debf0c8 in 12 100 MB part(s)"
time=2025-06-23T18:11:22.252Z level=INFO source=download.go:177 msg="downloading c5ad996bda6e in 1 556 B part(s)"
time=2025-06-23T18:11:23.573Z level=INFO source=download.go:177 msg="downloading 6e4c38e1172f in 1 1.1 KB part(s)"
time=2025-06-23T18:11:24.871Z level=INFO source=download.go:177 msg="downloading f4d24e9138dd in 1 148 B part(s)"
time=2025-06-23T18:11:26.178Z level=INFO source=download.go:177 msg="downloading a85fe2a2e58e in 1 487 B part(s)"
[GIN] 2025/06/23 - 18:11:27 | 200 | 18.649431745s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-23T18:11:27.797Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:11:27.896Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:11:27.903Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-23T18:11:27.903Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-23T18:11:27.903Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-23T18:11:27.903Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-23T18:11:27.903Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="1.9 GiB"
time=2025-06-23T18:11:27.990Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.2 GiB" free_swap="8.0 GiB"
time=2025-06-23T18:11:27.990Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-23T18:11:27.991Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-23T18:11:27.991Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-23T18:11:27.991Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.9 GiB" memory.required.partial="1.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[1.9 GiB]" memory.weights.total="934.7 MiB" memory.weights.repeating="752.1 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-23T18:11:28.097Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 34455"
time=2025-06-23T18:11:28.098Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-23T18:11:28.098Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-23T18:11:28.098Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-23T18:11:28.104Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-23T18:11:28.140Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-23T18:11:28.141Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:34455"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 1536
print_info: n_layer          = 28
print_info: n_head           = 12
print_info: n_head_kv        = 2
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 6
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8960
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.5B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   934.70 MiB
load_tensors:   CPU_Mapped model buffer size =   125.19 MiB
time=2025-06-23T18:11:28.349Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.17 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   224.00 MiB
llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   299.75 MiB
llama_context:  CUDA_Host compute buffer size =    19.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 2
time=2025-06-23T18:11:28.600Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/23 - 18:11:28 | 200 |  809.307743ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-23T18:11:34.611Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:37 | 200 |  2.452193611s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:37.068Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:37 | 200 |  503.733017ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:37.581Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:39 | 200 |  1.546969584s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:39.137Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:41 | 200 |  2.661392814s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:41.810Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:45 | 200 |  4.004493441s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:45.821Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:46 | 200 |  823.879422ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:46.654Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:48 | 200 |  1.361760307s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:48.025Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:49 | 200 |  1.162712377s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:49.197Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:52 | 200 |  3.555022763s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:52.761Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:54 | 200 |  1.717845976s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:54.488Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:57 | 200 |  3.332781001s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:57.834Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:11:59 | 200 |  1.205975759s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:11:59.049Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:01 | 200 |  1.964306481s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:01.028Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:01 | 200 |  463.385926ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:01.500Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:05 | 200 |  3.535754718s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:05.046Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:05 | 200 |   75.111209ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:05.129Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:06 | 200 |  1.177619498s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:06.315Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:06 | 200 |   82.079413ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:06.405Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:07 | 200 |  1.310256779s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:07.732Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:09 | 200 |  1.584932629s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:09.319Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:09 | 200 |  196.670136ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:09.524Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:10 | 200 |  1.316058852s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:10.849Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:11 | 200 |  477.952002ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:11.336Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:11 | 200 |  579.055257ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:11.923Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:13 | 200 |  1.113118036s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:13.046Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:14 | 200 |  1.417886741s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:14.472Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:14 | 200 |  250.903974ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:14.743Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:17 | 200 |  3.262811954s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:18.015Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:19 | 200 |  1.455188172s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:19.479Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:19 | 200 |  360.628349ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:19.849Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:21 | 200 |  2.028844647s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:21.886Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:24 | 200 |  3.065392756s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:24.969Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:27 | 200 |  2.900779389s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:27.878Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:36 | 200 |  8.434094424s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:12:36.322Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:12:38 | 200 |  2.041687239s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:36.153Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:40 | 200 |  4.726132656s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:40.890Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:42 | 200 |  1.866445964s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:42.765Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:44 | 200 |  1.495939572s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:44.270Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:46 | 200 |  2.111337953s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:46.390Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:49 | 200 |  3.229136729s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:49.628Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:50 | 200 |  1.202870943s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:50.839Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:52 | 200 |  1.556198985s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:52.405Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:53 | 200 |  1.178947094s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:53.592Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:55 | 200 |  1.731031216s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:55.332Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:16:57 | 200 |  2.300921482s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:16:57.641Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:02 | 200 |  4.854491341s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:02.513Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:03 | 200 |  844.133364ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:03.367Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:05 | 200 |  2.386310738s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:05.767Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:06 | 200 |  1.129470991s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:06.904Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:09 | 200 |  2.712073697s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:09.626Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:09 | 200 |  114.708961ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:09.749Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:09 | 200 |   90.275327ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:09.848Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:11 | 200 |  1.881422175s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:11.738Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:12 | 200 |  989.778097ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:12.737Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:16 | 200 |  3.338324678s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:16.085Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:16 | 200 |  316.770945ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:16.410Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:18 | 200 |  2.088858795s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:18.507Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:19 | 200 |  863.242579ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:19.379Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:19 | 200 |   355.51404ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:19.744Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:21 | 200 |   1.97224473s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:21.725Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:23 | 200 |  1.583599692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:23.318Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:25 | 200 |  2.437294858s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:25.764Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:30 | 200 |  4.626953443s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:30.402Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:32 | 200 |  2.022848926s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:32.432Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:33 | 200 |   724.55308ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:33.166Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:36 | 200 |   3.19074723s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:36.366Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:38 | 200 |  2.347437018s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:38.735Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:42 | 200 |  4.214275764s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:42.951Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:44 | 200 |  1.537195966s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-23T18:17:44.497Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/23 - 18:17:46 | 200 |   1.77971534s |       127.0.0.1 | POST     "/api/generate"
2025/06/24 13:38:53 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-24T13:38:53.940Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-24T13:38:53.940Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-24T13:38:53.941Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-24T13:38:53.941Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-24T13:38:54.098Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/24 - 13:39:03 | 200 |     439.399µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/24 - 13:39:03 | 200 |     321.895µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-24T13:39:03.140Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:39:03.239Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:39:03.246Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:39:03.246Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-24T13:39:03.246Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-24T13:39:03.246Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-24T13:39:03.246Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="1.9 GiB"
time=2025-06-24T13:39:03.333Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="121.0 GiB" free_swap="8.0 GiB"
time=2025-06-24T13:39:03.333Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-24T13:39:03.333Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-24T13:39:03.333Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-24T13:39:03.333Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.9 GiB" memory.required.partial="1.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[1.9 GiB]" memory.weights.total="934.7 MiB" memory.weights.repeating="752.1 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-24T13:39:03.440Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 39783"
time=2025-06-24T13:39:03.441Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-24T13:39:03.441Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-24T13:39:03.441Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-24T13:39:03.447Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-24T13:39:03.483Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-24T13:39:03.483Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:39783"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 1536
print_info: n_layer          = 28
print_info: n_head           = 12
print_info: n_head_kv        = 2
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 6
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8960
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.5B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-24T13:39:03.706Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   934.70 MiB
load_tensors:   CPU_Mapped model buffer size =   125.19 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.17 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   224.00 MiB
llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   299.75 MiB
llama_context:  CUDA_Host compute buffer size =    19.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 2
time=2025-06-24T13:39:03.958Z level=INFO source=server.go:628 msg="llama runner started in 0.52 seconds"
[GIN] 2025/06/24 - 13:39:03 | 200 |  827.418998ms |       127.0.0.1 | POST     "/api/chat"
2025/06/24 13:40:45 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-24T13:40:45.563Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-24T13:40:45.564Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-24T13:40:45.564Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-24T13:40:45.564Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-24T13:40:45.723Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/24 - 13:40:56 | 200 |     406.912µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/24 - 13:40:56 | 200 |     326.113µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-24T13:40:56.201Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:40:56.301Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:40:56.308Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:40:56.308Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:40:56.308Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="3.7 GiB"
time=2025-06-24T13:40:56.394Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.4 GiB" free_swap="8.0 GiB"
time=2025-06-24T13:40:56.394Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:40:56.394Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-24T13:40:56.531Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 39891"
time=2025-06-24T13:40:56.531Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-24T13:40:56.531Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-24T13:40:56.532Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-24T13:40:56.538Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-24T13:40:56.572Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-24T13:40:56.573Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:39891"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-24T13:40:56.782Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-24T13:40:57.034Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/24 - 13:40:57 | 200 |  846.765123ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-24T13:41:01.001Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:03 | 200 |  2.933618145s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:03.945Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:06 | 200 |  2.470183917s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:06.424Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:07 | 200 |  1.143671163s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:07.577Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:09 | 200 |  1.571713824s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:09.157Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:12 | 200 |  3.243890692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:12.410Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:12 | 200 |  186.737613ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:12.604Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:13 | 200 |  754.422178ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:13.372Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:13 | 200 |  450.484189ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:13.827Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:15 | 200 |  1.259321533s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:15.095Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:16 | 200 |  1.537400888s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:16.642Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:18 | 200 |   2.30816895s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:18.968Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:19 | 200 |  544.536508ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:19.521Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:20 | 200 |  975.118069ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:20.515Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:20 | 200 |  230.131766ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:20.746Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:23 | 200 |  3.259244746s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:24.014Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:24 | 200 |  327.281489ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:24.354Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:24 | 200 |  230.807858ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:24.590Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:27 | 200 |  2.567703602s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:27.166Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:28 | 200 |  1.470080371s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:28.690Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:31 | 200 |  2.753292964s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:31.444Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:34 | 200 |  2.856075195s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:34.308Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:34 | 200 |   139.46038ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:34.456Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:35 | 200 |  803.707287ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:35.269Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:38 | 200 |  2.771704736s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:38.050Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:39 | 200 |  998.155531ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:39.057Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:39 | 200 |  712.338104ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:39.783Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:40 | 200 |  497.113882ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:40.285Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:43 | 200 |  3.659231057s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:43.953Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:45 | 200 |  1.321818751s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:45.284Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:46 | 200 |  736.112216ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:46.029Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:46 | 200 |  910.500464ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:46.947Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:49 | 200 |  2.177338206s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:49.143Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:52 | 200 |  3.550870457s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:52.699Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:56 | 200 |   3.83295744s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:41:56.541Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:41:58 | 200 |  2.458606162s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/24 - 13:48:38 | 200 |     640.462µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/24 - 13:48:38 | 200 |      288.21µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-24T13:48:38.208Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:48:38.311Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:48:38.318Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:48:38.318Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:48:38.318Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15468789760 required="3.7 GiB"
time=2025-06-24T13:48:38.405Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.3 GiB" free_swap="8.0 GiB"
time=2025-06-24T13:48:38.405Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:48:38.405Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[14.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-24T13:48:38.542Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 41457"
time=2025-06-24T13:48:38.543Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-24T13:48:38.543Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-24T13:48:38.543Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-24T13:48:38.550Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-24T13:48:38.586Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-24T13:48:38.586Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:41457"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14752 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-24T13:48:38.794Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-24T13:48:39.046Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/24 - 13:48:39 | 200 |   846.09518ms |       127.0.0.1 | POST     "/api/chat"
2025/06/24 13:49:14 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-24T13:49:14.863Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-24T13:49:14.864Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-24T13:49:14.864Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-24T13:49:14.864Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-24T13:49:14.986Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="14.4 GiB"
time=2025-06-24T13:49:29.605Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:49:29.703Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:49:29.711Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-24T13:49:29.711Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:49:29.711Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15468789760 required="3.7 GiB"
time=2025-06-24T13:49:29.799Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.9 GiB" free_swap="8.0 GiB"
time=2025-06-24T13:49:29.799Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-24T13:49:29.800Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[14.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-24T13:49:29.933Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 35167"
time=2025-06-24T13:49:29.933Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-24T13:49:29.933Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-24T13:49:29.934Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-24T13:49:29.940Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-24T13:49:29.975Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-24T13:49:29.975Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:35167"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14752 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-24T13:49:30.206Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-24T13:49:30.467Z level=INFO source=server.go:628 msg="llama runner started in 0.53 seconds"
[GIN] 2025/06/24 - 13:49:33 | 200 |   3.82533489s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:33.439Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:36 | 200 |  2.934834013s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:36.384Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:39 | 200 |  2.890799038s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:39.283Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:40 | 200 |  1.299282462s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:40.590Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:43 | 200 |  3.150747844s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:43.751Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:43 | 200 |  223.895645ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:43.987Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:44 | 200 |   1.01313483s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:45.006Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:45 | 200 |  589.411365ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:45.604Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:46 | 200 |  1.243230682s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:46.861Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:48 | 200 |  1.927335489s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:48.792Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:51 | 200 |  2.278385462s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:51.091Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:51 | 200 |  185.613973ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:51.283Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:52 | 200 |  838.102731ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:52.133Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:52 | 200 |  285.988821ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:52.427Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:55 | 200 |  3.221976542s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:55.659Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:55 | 200 |  103.235502ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:55.770Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:56 | 200 |  243.054561ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:56.022Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:49:58 | 200 |  2.534530968s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:49:58.567Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:00 | 200 |  1.567273714s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:00.142Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:03 | 200 |  3.479010206s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:50.289Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:53 | 200 |  2.840395037s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:53.139Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:55 | 200 |  2.599779397s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:55.749Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:57 | 200 |  2.217911408s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:57.975Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:50:59 | 200 |  2.008980415s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:50:59.993Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:03 | 200 |  3.277395344s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:03.280Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:03 | 200 |  222.221244ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:03.511Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:04 | 200 |   753.92719ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:04.273Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:05 | 200 |   966.55606ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:05.248Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:06 | 200 |  1.185915442s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:06.443Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:08 | 200 |  1.793526736s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:08.245Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:10 | 200 |  2.509170486s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:10.765Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:10 | 200 |  205.197768ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:10.980Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:11 | 200 |  865.361976ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:11.856Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:12 | 200 |  222.449125ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:12.125Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:15 | 200 |  3.070377751s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:15.204Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:15 | 200 |   97.678243ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:15.306Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:15 | 200 |  214.614719ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:15.529Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:18 | 200 |  2.571600864s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:52.601Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:55 | 200 |  2.815008196s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:55.420Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:51:58 | 200 |  2.601559929s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:51:58.031Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:00 | 200 |  2.529909277s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:00.570Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:02 | 200 |  2.288324792s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:02.868Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:05 | 200 |  2.542897329s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:05.420Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:05 | 200 |  223.665482ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:05.652Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:06 | 200 |  678.058073ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:06.340Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:06 | 200 |  596.980204ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:06.945Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:08 | 200 |  1.960757517s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:08.915Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:12 | 200 |  3.679747664s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:12.605Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:14 | 200 |   1.84588358s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:14.462Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:14 | 200 |  138.395936ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:14.615Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:15 | 200 |  1.051710464s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:15.674Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:15 | 200 |  156.151314ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:15.837Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:18 | 200 |   3.08544793s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:18.932Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:19 | 200 |  112.278897ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:19.053Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:19 | 200 |  214.767436ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:52:19.276Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:52:22 | 200 |  2.826209395s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:15.851Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:18 | 200 |   2.81520452s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:18.676Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:21 | 200 |  3.148565345s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:21.836Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:23 | 200 |  1.865296966s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:23.708Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:25 | 200 |  2.048959465s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:25.765Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:29 | 200 |  3.578623784s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:29.353Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:29 | 200 |  312.381866ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:29.674Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:30 | 200 |  720.340974ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:30.403Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:30 | 200 |  511.665801ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:30.923Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:31 | 200 |  595.365155ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:31.527Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:33 | 200 |   2.31298788s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:33.850Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:35 | 200 |   2.11097747s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:35.970Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:36 | 200 |    77.07572ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:36.059Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:36 | 200 |  854.865205ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:36.924Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:37 | 200 |  156.334722ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:37.087Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:40 | 200 |  3.260462745s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:40.357Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:40 | 200 |  113.419969ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:40.480Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:40 | 200 |  116.680217ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:40.604Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:43 | 200 |  3.127192753s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:43.740Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:45 | 200 |  2.066614932s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:45.821Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:48 | 200 |  3.041845638s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:48.867Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:50 | 200 |  1.568032568s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:50.443Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:50 | 200 |  241.810416ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:50.693Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:51 | 200 |  697.161045ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:51.399Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:54 | 200 |  2.674839164s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:54.083Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:55 | 200 |  1.105243922s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:55.197Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:56 | 200 |  1.450916474s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:56.657Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:53:57 | 200 |  844.507217ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:53:57.510Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:02 | 200 |  5.018216641s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:02.538Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:05 | 200 |  2.548880645s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:05.097Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:06 | 200 |  1.129684295s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:06.239Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:06 | 200 |  425.901931ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:06.669Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:09 | 200 |  2.600117011s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:09.281Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:13 | 200 |  4.199457187s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:13.490Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:18 | 200 |  4.781580897s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-24T13:54:18.288Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/24 - 13:54:21 | 200 |  2.808420917s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 13:53:06 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T13:53:06.348Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T13:53:06.348Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T13:53:06.349Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T13:53:06.349Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T13:53:06.503Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 13:53:15 | 200 |     488.402µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 13:53:15 | 200 |     733.185µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T13:53:15.463Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T13:53:15.563Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T13:53:15.571Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T13:53:15.571Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T13:53:15.571Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="3.7 GiB"
time=2025-06-25T13:53:15.657Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-25T13:53:15.657Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T13:53:15.657Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T13:53:15.791Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 43619"
time=2025-06-25T13:53:15.791Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T13:53:15.791Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T13:53:15.792Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T13:53:15.798Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T13:53:15.832Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T13:53:15.833Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:43619"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T13:53:16.043Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-25T13:53:16.294Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/25 - 13:53:16 | 200 |  838.809281ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T13:53:32.684Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:35 | 200 |  2.904522652s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:35.606Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:38 | 200 |  2.430271505s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:38.038Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:39 | 200 |  1.447291461s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:39.494Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:42 | 200 |  2.748747839s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:42.298Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:45 | 200 |  3.471032222s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:45.776Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:45 | 200 |  117.185766ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:45.901Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:46 | 200 |  794.726359ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:46.705Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:47 | 200 |  577.610496ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:47.291Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:48 | 200 |  1.223319477s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:48.524Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:50 | 200 |  2.125180908s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:50.657Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:52 | 200 |  1.917294303s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:52.587Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:52 | 200 |  242.112293ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:52.846Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:53 | 200 |  928.794566ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:53.782Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:53 | 200 |  172.602776ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:53.957Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:57 | 200 |  3.447242066s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:57.413Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:57 | 200 |   92.841359ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:57.518Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:53:57 | 200 |  243.102207ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:53:57.767Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:00 | 200 |  2.551151795s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:00.326Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:01 | 200 |  1.013956584s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:01.350Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:04 | 200 |  3.270340257s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:04.628Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:07 | 200 |  2.432662072s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:07.069Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:07 | 200 |  233.827303ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:07.312Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:07 | 200 |  560.498155ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:07.886Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:10 | 200 |  2.887238578s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:10.777Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:11 | 200 |  1.145755908s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:11.933Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:12 | 200 |  1.064315175s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:13.006Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:13 | 200 |   457.20789ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:13.476Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:16 | 200 |  3.050232623s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:16.531Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:17 | 200 |  1.149694858s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:17.690Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:18 | 200 |  547.151622ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:18.247Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:18 | 200 |  596.554425ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:18.852Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:21 | 200 |  2.804687775s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:21.672Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:24 | 200 |  3.233923101s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:24.915Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:29 | 200 |  4.707021198s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T13:54:29.632Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 13:54:32 | 200 |  2.793245596s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:02:23 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:02:23.791Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:02:23.792Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:02:23.793Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:02:23.793Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:02:23.948Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:02:33 | 200 |     443.926µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:02:33 | 200 |     339.752µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:02:33.975Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:02:34.075Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:02:34.082Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:02:34.082Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:02:34.083Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="3.7 GiB"
time=2025-06-25T14:02:34.167Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:02:34.167Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:02:34.167Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="1.9 GiB" memory.weights.repeating="1.6 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:02:34.302Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 40241"
time=2025-06-25T14:02:34.303Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:02:34.303Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:02:34.303Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:02:34.309Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:02:34.344Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:02:34.344Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:40241"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/uo289183/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:02:34.554Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1918.35 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.00 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   896.00 MiB
llama_context: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context:      CUDA0 compute buffer size =   424.00 MiB
llama_context:  CUDA_Host compute buffer size =    22.01 MiB
llama_context: graph nodes  = 958
llama_context: graph splits = 2
time=2025-06-25T14:02:34.805Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/25 - 14:02:34 | 200 |  838.694447ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:02:42.817Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:45 | 200 |  2.492550736s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:45.320Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:48 | 200 |   3.04163378s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:48.371Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:50 | 200 |  1.940861741s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:50.321Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:52 | 200 |  2.268953529s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:52.603Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:56 | 200 |  3.497436436s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:56.105Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:56 | 200 |  206.791995ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:56.321Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:56 | 200 |  680.920175ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:57.011Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:57 | 200 |  801.132815ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:57.821Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:02:58 | 200 |  537.392876ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:02:58.366Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:03 | 200 |  4.810188025s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:03.185Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:05 | 200 |  2.194622687s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:05.397Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:05 | 200 |  171.374137ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:05.574Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:06 | 200 |  684.187181ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:06.268Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:06 | 200 |  574.699848ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:06.851Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:09 | 200 |  3.151635426s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:10.013Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:10 | 200 |   95.163072ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:10.116Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:10 | 200 |  230.192784ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:10.355Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:12 | 200 |  2.009787293s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:12.375Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:14 | 200 |  1.700767403s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:14.084Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:16 | 200 |  2.865615119s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:16.959Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:18 | 200 |  1.910606964s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:18.879Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:19 | 200 |  151.004818ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:19.038Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:19 | 200 |  717.046831ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:19.764Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:22 | 200 |  2.853232897s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:22.627Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:25 | 200 |  2.675599959s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:25.311Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:26 | 200 |  1.045213587s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:26.365Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:27 | 200 |  1.079495532s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:27.454Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:31 | 200 |  4.033489203s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:31.497Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:32 | 200 |  1.141457277s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:32.647Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:33 | 200 |  756.406195ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:33.412Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:35 | 200 |   2.10248782s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:35.524Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:38 | 200 |  3.475341682s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:39.022Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:41 | 200 |  2.866959087s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:41.891Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:46 | 200 |  4.277282612s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:03:46.178Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:03:48 | 200 |  2.670098912s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:16:47 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:16:47.529Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:16:47.529Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:16:47.529Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:16:47.530Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:16:47.678Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:16:52 | 200 |     591.796µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:16:52 | 200 |     326.089µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:16:52.547Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:16:52.647Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:16:52.654Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:16:52.654Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:16:52.654Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-25T14:16:52.654Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-25T14:16:52.654Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="6.5 GiB"
time=2025-06-25T14:16:52.739Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:16:52.739Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:16:52.739Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-25T14:16:52.739Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-25T14:16:52.740Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.5 GiB" memory.required.partial="6.5 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.5 GiB]" memory.weights.total="4.3 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:16:52.888Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 2 --port 44187"
time=2025-06-25T14:16:52.888Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:16:52.888Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:16:52.889Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:16:52.895Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:16:52.930Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:16:52.931Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:44187"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /home/uo289183/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:16:53.139Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4403.49 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.01 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
init:      CUDA0 KV buffer size =  1024.00 MiB
llama_context: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 2
time=2025-06-25T14:16:53.641Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/25 - 14:16:53 | 200 |  1.102606466s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:16:58.680Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:03 | 200 |  4.387663965s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:03.075Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:04 | 200 |  997.435802ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:04.082Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:04 | 200 |  435.059612ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:04.525Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:05 | 200 |  573.388675ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:05.108Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:09 | 200 |  3.923926092s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:09.041Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:09 | 200 |   82.164255ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:09.135Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:09 | 200 |  656.111318ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:09.796Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:10 | 200 |  951.243849ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:10.756Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:11 | 200 |  569.007878ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:11.334Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:11 | 200 |   93.201706ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:11.435Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:12 | 200 |  1.476893649s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:12.928Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:13 | 200 |  140.232744ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:13.079Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:14 | 200 |  1.284966565s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:14.374Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:14 | 200 |  307.817915ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:14.690Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:19 | 200 |  5.115076526s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:19.820Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:19 | 200 |  164.550534ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:19.988Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:20 | 200 |  487.059186ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:20.484Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:23 | 200 |  2.524813464s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:23.018Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:27 | 200 |  4.533774954s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:27.561Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:32 | 200 |  4.676433248s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:32.246Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:39 | 200 |  7.100768692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:39.356Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:39 | 200 |  271.208377ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:39.636Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:39 | 200 |  304.767453ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:39.949Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:43 | 200 |  3.979171854s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:43.937Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:49 | 200 |  5.730779337s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:49.678Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:49 | 200 |  126.265985ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:49.811Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:50 | 200 |  620.759973ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:50.441Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:52 | 200 |  1.847038613s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:52.297Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:53 | 200 |  890.890094ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:53.197Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:53 | 200 |   91.041435ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:53.297Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:17:56 | 200 |  2.947372957s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:17:56.252Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:18:01 | 200 |  5.380280378s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:18:01.657Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:18:07 | 200 |  5.629461599s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:18:07.288Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:18:14 | 200 |  7.690784715s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:18:14.989Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:18:19 | 200 |  4.053250058s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:21:13 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:21:13.958Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:21:13.958Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:21:13.959Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:21:13.959Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:21:14.104Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
2025/06/25 14:21:51 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:21:51.815Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:21:51.815Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:21:51.816Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:21:51.816Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:21:51.967Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:21:58 | 200 |     456.679µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:21:58 | 200 |     316.374µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:21:58.698Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:21:58.799Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:21:58.806Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:21:58.806Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:21:58.806Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-25T14:21:58.806Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-25T14:21:58.807Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="6.2 GiB"
time=2025-06-25T14:21:58.898Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:21:58.898Z level=WARN source=ggml.go:152 msg="key not found" key=llama.vision.block_count default=0
time=2025-06-25T14:21:58.898Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.key_length default=128
time=2025-06-25T14:21:58.898Z level=WARN source=ggml.go:152 msg="key not found" key=llama.attention.value_length default=128
time=2025-06-25T14:21:58.898Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:21:59.034Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 2 --port 45419"
time=2025-06-25T14:21:59.034Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:21:59.034Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:21:59.034Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:21:59.041Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:21:59.078Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:21:59.078Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:45419"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/uo289183/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:21:59.375Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.01 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
init:      CUDA0 KV buffer size =  1024.00 MiB
llama_context: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 2
time=2025-06-25T14:21:59.878Z level=INFO source=server.go:628 msg="llama runner started in 0.84 seconds"
[GIN] 2025/06/25 - 14:21:59 | 200 |  1.188725942s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:22:03.459Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:08 | 200 |  4.662685837s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:08.128Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:13 | 200 |   5.13890509s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:13.277Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:17 | 200 |  4.474500223s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:17.761Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:21 | 200 |  3.848010347s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:21.662Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:27 | 200 |  5.557237095s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:27.223Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:27 | 200 |  227.353718ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:27.459Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:28 | 200 |  1.113514772s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:28.581Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:29 | 200 |  544.412919ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:29.138Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:30 | 200 |  1.581713903s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:30.724Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:35 | 200 |  4.997840873s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:35.731Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:38 | 200 |  3.177038346s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:38.919Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:39 | 200 |  468.468993ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:39.396Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:40 | 200 |  1.251090752s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:40.658Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:41 | 200 |  481.071855ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:41.149Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:46 | 200 |  5.169176343s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:46.326Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:46 | 200 |  551.694655ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:46.886Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:47 | 200 |  486.365143ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:47.382Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:51 | 200 |  4.595698144s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:51.987Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:55 | 200 |  3.131402409s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:55.127Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:22:59 | 200 |  3.979942055s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:22:59.116Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:01 | 200 |  2.762462424s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:01.887Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:03 | 200 |  1.793182858s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:03.690Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:05 | 200 |   1.42147368s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:05.120Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:08 | 200 |  3.783115881s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:08.912Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:13 | 200 |  4.930809595s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:13.852Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:14 | 200 |  1.108100254s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:14.969Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:17 | 200 |  2.220490652s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:17.198Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:23 | 200 |  6.708070492s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:23.921Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:30 | 200 |  6.286991494s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:30.211Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:30 | 200 |  774.105145ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:30.994Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:32 | 200 |  1.813049237s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:32.816Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:36 | 200 |  3.670173345s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:36.498Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:39 | 200 |  3.411153836s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:39.916Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:46 | 200 |  6.690596692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:23:46.616Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:23:51 | 200 |  4.476909594s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:36:27 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:36:27.790Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:36:27.790Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:36:27.790Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:36:27.790Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:36:27.945Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:36:33 | 200 |     407.562µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:36:33 | 200 |     342.789µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:36:33.441Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:36:33.541Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:36:33.548Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:36:33.548Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-25T14:36:33.548Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-25T14:36:33.548Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-25T14:36:33.548Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="1.9 GiB"
time=2025-06-25T14:36:33.634Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.7 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:36:33.634Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-25T14:36:33.634Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-25T14:36:33.634Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-25T14:36:33.634Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.9 GiB" memory.required.partial="1.9 GiB" memory.required.kv="224.0 MiB" memory.required.allocations="[1.9 GiB]" memory.weights.total="934.7 MiB" memory.weights.repeating="752.1 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:36:33.741Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 8 --parallel 2 --port 35043"
time=2025-06-25T14:36:33.741Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:36:33.741Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:36:33.741Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:36:33.747Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:36:33.783Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:36:33.783Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:35043"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/uo289183/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.04 GiB (5.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 1536
print_info: n_layer          = 28
print_info: n_head           = 12
print_info: n_head_kv        = 2
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 6
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8960
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.5B
print_info: model params     = 1.78 B
print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   934.70 MiB
load_tensors:   CPU_Mapped model buffer size =   125.19 MiB
time=2025-06-25T14:36:33.993Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.17 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
init:      CUDA0 KV buffer size =   224.00 MiB
llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      CUDA0 compute buffer size =   299.75 MiB
llama_context:  CUDA_Host compute buffer size =    19.01 MiB
llama_context: graph nodes  = 1042
llama_context: graph splits = 2
time=2025-06-25T14:36:34.244Z level=INFO source=server.go:628 msg="llama runner started in 0.50 seconds"
[GIN] 2025/06/25 - 14:36:34 | 200 |   816.38486ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:36:41.354Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:44 | 200 |  3.286296152s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:44.654Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:45 | 200 |   940.20551ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:45.600Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:45 | 200 |  362.610471ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:45.971Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:48 | 200 |  2.055956008s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:48.036Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:50 | 200 |  2.878299284s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:50.924Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:52 | 200 |  1.413323274s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:52.346Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:53 | 200 |  1.223902096s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:53.579Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:54 | 200 |  682.807297ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:54.273Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:56 | 200 |  2.125558646s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:56.405Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:36:58 | 200 |  2.567913719s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:36:58.981Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:02 | 200 |   3.49751126s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:02.495Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:03 | 200 |  1.304367692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:03.810Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:06 | 200 |  2.575174213s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:06.394Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:07 | 200 |  668.996589ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:07.071Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:10 | 200 |  3.349022692s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:10.430Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:10 | 200 |   85.985159ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:10.525Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:11 | 200 |  974.121276ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:11.508Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:11 | 200 |  185.402914ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:11.701Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:13 | 200 |  1.877644408s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:13.588Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:21 | 200 |  7.621801542s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:21.219Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:21 | 200 |  266.294797ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:21.495Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:21 | 200 |  229.495571ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:21.733Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:22 | 200 |  430.546168ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:22.172Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:24 | 200 |  2.437877538s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:24.620Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:26 | 200 |  2.120799147s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:26.749Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:28 | 200 |  1.541554944s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:28.299Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:29 | 200 |  1.529550393s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:29.838Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:30 | 200 |  581.507776ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:30.429Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:31 | 200 |  1.496315785s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:31.933Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:32 | 200 |  497.792739ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:32.439Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:34 | 200 |  2.461897454s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:34.911Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:37 | 200 |  2.803331005s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:37.726Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:44 | 200 |  6.683935658s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:44.420Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:51 | 200 |  6.802873418s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:37:51.232Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:37:56 | 200 |  5.613217119s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:43:54 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:43:54.779Z level=INFO source=images.go:463 msg="total blobs: 32"
time=2025-06-25T14:43:54.780Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:43:54.780Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:43:54.780Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:43:54.896Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="14.4 GiB"
[GIN] 2025/06/25 - 14:43:59 | 200 |     416.157µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:43:59 | 200 |     477.993µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:43:59.917Z level=INFO source=download.go:177 msg="downloading e6a7edc1a4d7 in 16 326 MB part(s)"
time=2025-06-25T14:44:47.253Z level=INFO source=download.go:177 msg="downloading ed8474dc73db in 1 179 B part(s)"
time=2025-06-25T14:44:48.537Z level=INFO source=download.go:177 msg="downloading f64cd5418e4b in 1 487 B part(s)"
[GIN] 2025/06/25 - 14:44:51 | 200 | 52.726011125s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-25T14:44:51.933Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:44:52.036Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:44:52.043Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:44:52.043Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:44:52.043Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15468789760 required="7.0 GiB"
time=2025-06-25T14:44:52.132Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.8 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:44:52.132Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:44:52.132Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[14.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.0 GiB" memory.required.partial="7.0 GiB" memory.required.kv="1.1 GiB" memory.required.allocations="[7.0 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="768.0 MiB" memory.graph.partial="768.0 MiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:44:52.241Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 2 --port 45901"
time=2025-06-25T14:44:52.242Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:44:52.242Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:44:52.242Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:44:52.249Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:44:52.284Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:44:52.285Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:45901"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14752 MiB free
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:44:52.493Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.19 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1
init:      CUDA0 KV buffer size =  1152.00 MiB
llama_context: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-06-25T14:44:52.995Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/25 - 14:44:52 | 200 |   1.06817892s |       127.0.0.1 | POST     "/api/chat"
2025/06/25 14:46:16 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:46:16.766Z level=INFO source=images.go:463 msg="total blobs: 35"
time=2025-06-25T14:46:16.767Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:46:16.767Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:46:16.767Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:46:16.925Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:46:21 | 200 |     473.168µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:46:21 | 200 |     396.279µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:46:21.578Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:46:21.684Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:46:21.695Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:46:21.695Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:46:21.696Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="7.0 GiB"
time=2025-06-25T14:46:21.790Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.6 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:46:21.790Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:46:21.790Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.0 GiB" memory.required.partial="7.0 GiB" memory.required.kv="1.1 GiB" memory.required.allocations="[7.0 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="768.0 MiB" memory.graph.partial="768.0 MiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:46:21.899Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 2 --port 34205"
time=2025-06-25T14:46:21.900Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:46:21.900Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:46:21.900Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:46:21.909Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:46:21.951Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:46:21.951Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:34205"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:46:22.152Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.19 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1
init:      CUDA0 KV buffer size =  1152.00 MiB
llama_context: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-06-25T14:46:22.653Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/25 - 14:46:22 | 200 |  1.082139074s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:46:29.486Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:46:44 | 200 | 14.849187478s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:46:44.345Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:46:52 | 200 |  8.606759965s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:46:52.961Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:47:00 | 200 |  7.092340823s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:47:00.063Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:47:10 | 200 | 10.657041708s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:47:10.729Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:47:18 | 200 |  7.818589899s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:47:18.556Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:55:27 | 500 |          8m8s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 14:55:35 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T14:55:35.973Z level=INFO source=images.go:463 msg="total blobs: 35"
time=2025-06-25T14:55:35.974Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T14:55:35.974Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T14:55:35.974Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T14:55:36.131Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 14:55:40 | 200 |      559.99µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 14:55:40 | 200 |     327.944µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T14:55:40.903Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:55:41.002Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:55:41.008Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T14:55:41.008Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:55:41.009Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="7.0 GiB"
time=2025-06-25T14:55:41.094Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.5 GiB" free_swap="8.0 GiB"
time=2025-06-25T14:55:41.094Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T14:55:41.094Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.0 GiB" memory.required.partial="7.0 GiB" memory.required.kv="1.1 GiB" memory.required.allocations="[7.0 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="768.0 MiB" memory.graph.partial="768.0 MiB"
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T14:55:41.204Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 2 --port 33135"
time=2025-06-25T14:55:41.204Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T14:55:41.204Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T14:55:41.205Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T14:55:41.214Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T14:55:41.257Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T14:55:41.257Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:33135"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 32 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-e6a7edc1a4d7d9b2de136a221a57336b76316cfe53a252aeba814496c5ae439d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 0528 Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-0528-Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 131072
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                    qwen3.rope.scaling.type str              = yarn
llama_model_loader: - kv  17:                  qwen3.rope.scaling.factor f32              = 4.000000
llama_model_loader: - kv  18: qwen3.rope.scaling.original_context_length u32              = 32768
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 151645
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 28
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 0.25
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = DeepSeek R1 0528 Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T14:55:41.526Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 0.25
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.19 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1
init:      CUDA0 KV buffer size =  1152.00 MiB
llama_context: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-06-25T14:55:42.029Z level=INFO source=server.go:628 msg="llama runner started in 0.82 seconds"
[GIN] 2025/06/25 - 14:55:42 | 200 |  1.132216194s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T14:55:46.045Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:55:53 | 200 |  7.510183574s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:55:53.565Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:03 | 200 |  9.572510901s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:03.147Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:08 | 200 |  4.946543382s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:08.102Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:14 | 200 |  6.898687058s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:15.012Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:25 | 200 | 10.170569543s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:25.190Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:34 | 200 |  9.384041693s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:34.582Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:56:51 | 200 | 16.739928963s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:56:51.335Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:57:12 | 200 |  21.03001364s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:57:12.379Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:57:32 | 200 | 20.320125466s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:57:32.717Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:57:45 | 200 |  13.29412918s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:57:46.023Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:57:58 | 200 | 12.225153748s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:57:58.261Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:04 | 200 |  6.000926253s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:04.273Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:09 | 200 |  5.152655826s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:09.433Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:16 | 200 |  6.664556402s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:16.103Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:31 | 200 | 15.455092741s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:31.579Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:34 | 200 |  3.295165236s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:34.878Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:41 | 200 |  6.753795673s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:41.642Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:46 | 200 |  4.710977917s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:46.362Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:58:55 | 200 |  9.105156213s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:58:55.480Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:02 | 200 |  6.938123084s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:02.423Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:07 | 200 |  5.124576056s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:07.556Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:11 | 200 |  3.758707981s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:11.324Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:15 | 200 |  4.508767413s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:15.845Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:21 | 200 |   6.00364973s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:21.854Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:28 | 200 |   6.77724497s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:28.640Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:31 | 200 |  3.099482357s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:31.748Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:37 | 200 |  5.989685284s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:37.748Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:40 | 200 |  2.748263177s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:40.504Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:44 | 200 |  4.468132509s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:44.981Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:49 | 200 |  4.191562281s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:49.182Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 14:59:56 | 200 |  7.247611585s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T14:59:56.438Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:00:22 | 200 | 25.732952499s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:00:22.189Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:00:54 | 200 | 32.555516364s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:00:54.755Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:01:34 | 200 | 40.041731145s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:01:34.808Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:01:48 | 200 | 13.926107472s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:09:06 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:09:06.703Z level=INFO source=images.go:463 msg="total blobs: 35"
time=2025-06-25T15:09:06.703Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:09:06.703Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:09:06.703Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:09:06.856Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:09:12 | 200 |     428.677µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:09:12 | 200 |     932.717µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:09:13.092Z level=INFO source=download.go:177 msg="downloading 6e9f90f02bb3 in 16 561 MB part(s)"
time=2025-06-25T15:10:33.381Z level=INFO source=download.go:177 msg="downloading 3c24b0c80794 in 1 488 B part(s)"
[GIN] 2025/06/25 - 15:10:38 | 200 |         1m25s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-25T15:10:38.416Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:10:38.515Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:10:38.522Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:10:38.522Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-25T15:10:38.522Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-25T15:10:38.522Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-25T15:10:38.522Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="10.8 GiB"
time=2025-06-25T15:10:38.607Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.5 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:10:38.607Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.vision.block_count default=0
time=2025-06-25T15:10:38.607Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-06-25T15:10:38.607Z level=WARN source=ggml.go:152 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-06-25T15:10:38.607Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="10.8 GiB" memory.required.partial="10.8 GiB" memory.required.kv="1.5 GiB" memory.required.allocations="[10.8 GiB]" memory.weights.total="8.0 GiB" memory.weights.repeating="7.4 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
llama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from /home/uo289183/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 48
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  241 tensors
llama_model_loader: - type q4_K:  289 tensors
llama_model_loader: - type q6_K:   49 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.37 GiB (4.87 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = DeepSeek R1 Distill Qwen 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T15:10:38.715Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 8 --parallel 2 --port 39277"
time=2025-06-25T15:10:38.716Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:10:38.716Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:10:38.716Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:10:38.722Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:10:38.757Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:10:38.758Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:39277"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from /home/uo289183/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 48
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  241 tensors
llama_model_loader: - type q4_K:  289 tensors
llama_model_loader: - type q6_K:   49 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.37 GiB (4.87 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 5120
print_info: n_layer          = 48
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 13824
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = DeepSeek R1 Distill Qwen 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'
print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T15:10:38.967Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 48 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 49/49 layers to GPU
load_tensors:        CUDA0 model buffer size =  8148.38 MiB
load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1
init:      CUDA0 KV buffer size =  1536.00 MiB
llama_context: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1782
llama_context: graph splits = 2
time=2025-06-25T15:10:39.972Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/25 - 15:10:39 | 200 |  1.563335615s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:11:33.903Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:11:49 | 200 | 16.048975511s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:11:49.957Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:11:57 | 200 |  7.547968026s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:11:57.515Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:08 | 200 | 11.000806631s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:08.525Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:18 | 200 |  10.43623802s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:18.970Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:34 | 200 |  15.74448098s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:34.726Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:40 | 200 |  5.923091492s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:40.659Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:47 | 200 |  6.534853735s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:47.203Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:52 | 200 |  5.402360055s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:52.614Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:12:57 | 200 |   5.31464576s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:12:57.938Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:13:07 | 200 |  9.178157442s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:13:07.125Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:13:28 | 200 |  21.58507282s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:13:28.735Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:13:38 | 200 |  9.873482587s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:13:38.621Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:13:45 | 200 |  7.027741249s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:13:45.661Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:13:52 | 200 |  6.352791353s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:13:52.022Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:04 | 200 |   12.4158849s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:04.448Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:05 | 200 |  1.085300691s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:05.542Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:08 | 200 |  3.312469021s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:08.864Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:15 | 200 |  6.143074013s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:15.016Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:19 | 200 |  4.838536539s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:19.863Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:32 | 200 | 12.681433712s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:32.554Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:42 | 200 |  9.463262512s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:42.026Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:44 | 200 |  2.606283782s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:44.645Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:47 | 200 |  2.637806948s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:47.290Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:14:58 | 200 | 10.787814143s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:14:58.087Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:07 | 200 |  9.173315608s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:07.269Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:09 | 200 |  1.977659374s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:09.256Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:14 | 200 |  5.109812029s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:14.375Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:27 | 200 | 12.675456666s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:27.059Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:31 | 200 |  4.791680123s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:31.860Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:35 | 200 |  3.559953557s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:35.437Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:15:45 | 200 |  9.846118642s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:15:45.285Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:16:00 | 200 | 15.482923223s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:16:00.782Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:16:19 | 200 | 18.913392183s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:16:19.708Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:16:49 | 200 | 29.977568203s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:16:49.702Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:17:06 | 200 |  16.37005172s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:21:04 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:21:04.988Z level=INFO source=images.go:463 msg="total blobs: 37"
time=2025-06-25T15:21:04.990Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:21:04.991Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:21:04.991Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:21:05.153Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:21:10 | 200 |     501.625µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:21:10 | 200 |    1.054099ms |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:21:10.683Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:21:10.794Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:21:10.812Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:21:10.813Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="5.6 GiB"
time=2025-06-25T15:21:10.900Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.5 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:21:10.901Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.6 GiB" memory.required.partial="5.6 GiB" memory.required.kv="450.0 MiB" memory.required.allocations="[5.6 GiB]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-25T15:21:10.933Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:21:10.934Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-25T15:21:10.936Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-25T15:21:10.936Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-25T15:21:10.936Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-25T15:21:10.936Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-25T15:21:10.936Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-25T15:21:10.936Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 8192 --batch-size 512 --n-gpu-layers 35 --threads 8 --parallel 2 --port 37851"
time=2025-06-25T15:21:10.937Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:21:10.937Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:21:10.937Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:21:10.943Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-25T15:21:10.944Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:37851"
time=2025-06-25T15:21:10.990Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:21:10.991Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-25T15:21:10.991Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-25T15:21:10.991Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:21:11.030Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:21:11.085Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="3.1 GiB"
time=2025-06-25T15:21:11.085Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="525.0 MiB"
time=2025-06-25T15:21:11.188Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-25T15:21:11.911Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-25T15:21:11.913Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-25T15:21:11.913Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-25T15:21:11.913Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-25T15:21:11.913Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-25T15:21:11.913Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-25T15:21:11.935Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="158.0 MiB"
time=2025-06-25T15:21:11.935Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="5.0 MiB"
time=2025-06-25T15:21:11.941Z level=INFO source=server.go:628 msg="llama runner started in 1.00 seconds"
[GIN] 2025/06/25 - 15:21:11 | 200 |  1.281162202s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:21:15.791Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:20 | 200 |  4.612037579s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:20.417Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:23 | 200 |  2.610680495s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:23.033Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:27 | 200 |  4.059967339s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:27.101Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:29 | 200 |  2.639427246s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:29.750Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:35 | 200 |  5.331143858s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:35.095Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:36 | 200 |  1.064491824s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:36.166Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:37 | 200 |  907.408795ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:37.082Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:37 | 200 |  515.167072ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:37.605Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:39 | 200 |  1.571625128s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:39.187Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:41 | 200 |  2.378058286s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:41.576Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:45 | 200 |  3.599853881s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:45.194Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:46 | 200 |  1.455417768s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:46.650Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:47 | 200 |  821.647409ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:47.482Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:48 | 200 |  1.456389397s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:48.947Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:54 | 200 |  5.100178128s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:54.063Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:54 | 200 |   367.41068ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:54.433Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:54 | 200 |  254.175059ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:54.696Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:21:59 | 200 |  5.048508294s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:21:59.752Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:03 | 200 |  3.951333644s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:03.751Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:07 | 200 |  3.794635932s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:07.554Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:12 | 200 |  4.862320287s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:12.432Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:15 | 200 |  3.501118443s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:15.936Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:19 | 200 |  3.778566364s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:19.724Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:24 | 200 |  4.995846389s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:24.728Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:32 | 200 |  7.769329417s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:32.507Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:32 | 200 |  249.923498ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:32.766Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:33 | 200 |  667.788098ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:33.445Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:41 | 200 |  7.906435184s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:41.360Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:47 | 200 |  6.112339851s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:47.479Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:47 | 200 |  240.002952ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:47.727Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:53 | 200 |  5.295175279s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:53.031Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:22:59 | 200 |  6.556529786s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:22:59.604Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:23:07 | 200 |  7.473786314s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:23:07.096Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:23:16 | 200 |  9.493025992s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:23:16.587Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:23:21 | 200 |  5.155600511s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:27:06 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:27:06.845Z level=INFO source=images.go:463 msg="total blobs: 37"
time=2025-06-25T15:27:06.846Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:27:06.846Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:27:06.846Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:27:06.990Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:27:11 | 200 |     501.108µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:27:11 | 200 |     789.669µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:27:11.744Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:27:11.858Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:27:11.875Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:27:11.877Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="11.0 GiB"
time=2025-06-25T15:27:11.963Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.5 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:27:11.964Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=49 layers.offload=49 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.0 GiB" memory.required.partial="11.0 GiB" memory.required.kv="1.3 GiB" memory.required.allocations="[11.0 GiB]" memory.weights.total="6.8 GiB" memory.weights.repeating="6.0 GiB" memory.weights.nonrepeating="787.5 MiB" memory.graph.full="519.5 MiB" memory.graph.partial="1.3 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-06-25T15:27:11.997Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:27:11.998Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-25T15:27:12.000Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-25T15:27:12.000Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-25T15:27:12.000Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-25T15:27:12.000Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-25T15:27:12.000Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-25T15:27:12.000Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --ollama-engine --model /home/uo289183/.ollama/models/blobs/sha256-e8ad13eff07a78d89926e9e8b882317d082ef5bf9768ad7b50fcdbbcd63748de --ctx-size 8192 --batch-size 512 --n-gpu-layers 49 --threads 8 --parallel 2 --port 43023"
time=2025-06-25T15:27:12.000Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:27:12.000Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:27:12.001Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:27:12.009Z level=INFO source=runner.go:851 msg="starting ollama engine"
time=2025-06-25T15:27:12.009Z level=INFO source=runner.go:914 msg="Server listening on 127.0.0.1:43023"
time=2025-06-25T15:27:12.045Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:27:12.046Z level=WARN source=ggml.go:152 msg="key not found" key=general.name default=""
time=2025-06-25T15:27:12.046Z level=WARN source=ggml.go:152 msg="key not found" key=general.description default=""
time=2025-06-25T15:27:12.046Z level=INFO source=ggml.go:72 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=1065 num_key_values=37
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:27:12.083Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:27:12.138Z level=INFO source=ggml.go:298 msg="model weights" buffer=CUDA0 size="7.6 GiB"
time=2025-06-25T15:27:12.138Z level=INFO source=ggml.go:298 msg="model weights" buffer=CPU size="787.5 MiB"
time=2025-06-25T15:27:12.253Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
time=2025-06-25T15:27:16.369Z level=WARN source=ggml.go:152 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-06-25T15:27:16.371Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-06-25T15:27:16.371Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-06-25T15:27:16.371Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-06-25T15:27:16.371Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-06-25T15:27:16.371Z level=WARN source=ggml.go:152 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-06-25T15:27:16.393Z level=INFO source=ggml.go:553 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="308.0 MiB"
time=2025-06-25T15:27:16.393Z level=INFO source=ggml.go:553 msg="compute graph" backend=CPU buffer_type=CPU size="7.5 MiB"
time=2025-06-25T15:27:16.527Z level=INFO source=server.go:628 msg="llama runner started in 4.53 seconds"
[GIN] 2025/06/25 - 15:27:16 | 200 |  4.818524494s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:27:20.021Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:27:30 | 200 | 10.541110288s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:27:30.574Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:27:37 | 200 |  6.593059604s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:27:37.174Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:27:44 | 200 |  7.004142344s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:27:44.188Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:27:51 | 200 |  7.470854484s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:27:51.668Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:05 | 200 | 13.816324686s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:05.497Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:05 | 200 |  226.487848ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:05.731Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:08 | 200 |  2.468324268s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:08.209Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:09 | 200 |  1.114396543s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:09.331Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:11 | 200 |   2.11960838s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:11.459Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:18 | 200 |  7.321914732s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:18.790Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:36 | 200 |  18.01109001s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:36.815Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:37 | 200 |  1.108791983s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:37.941Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:39 | 200 |  1.761678522s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:39.712Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:43 | 200 |  3.707106101s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:43.432Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:57 | 200 | 14.331657772s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:57.763Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:58 | 200 |  614.736968ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:58.387Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:28:59 | 200 |  678.818832ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:28:59.075Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:11 | 200 | 12.211971464s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:11.296Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:22 | 200 |   10.8452084s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:22.149Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:30 | 200 |  8.017138305s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:30.188Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:40 | 200 | 10.723878058s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:40.911Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:49 | 200 |  8.629635471s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:49.548Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:29:55 | 200 |   5.71837492s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:29:55.276Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:30:07 | 200 | 12.304745205s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:30:07.590Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:30:27 | 200 | 20.262485483s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:30:27.863Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:30:28 | 200 |  544.715136ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:30:28.419Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:30:29 | 200 |  1.159122176s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:30:29.586Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:30:48 | 200 | 19.254315082s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:30:48.852Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:31:03 | 200 | 14.649171699s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:31:03.510Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:31:03 | 200 |  447.006101ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:31:03.968Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:31:15 | 200 | 12.033544525s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:31:16.011Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:31:30 | 200 | 14.926777348s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:31:30.961Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:31:48 | 200 | 17.973711464s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:31:48.936Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:32:16 | 200 | 28.000636207s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:32:16.946Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:32:33 | 200 | 16.633560896s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:36:12 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:36:12.985Z level=INFO source=images.go:463 msg="total blobs: 37"
time=2025-06-25T15:36:12.985Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:36:12.986Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:36:12.986Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:36:13.143Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:36:18 | 200 |     474.814µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:36:18 | 200 |     374.118µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:36:19.134Z level=INFO source=download.go:177 msg="downloading 163553aea1b1 in 16 163 MB part(s)"
time=2025-06-25T15:36:44.454Z level=INFO source=download.go:177 msg="downloading ae370d884f10 in 1 1.7 KB part(s)"
time=2025-06-25T15:36:45.736Z level=INFO source=download.go:177 msg="downloading d18a5cc71b84 in 1 11 KB part(s)"
time=2025-06-25T15:36:47.019Z level=INFO source=download.go:177 msg="downloading cff3f395ef37 in 1 120 B part(s)"
time=2025-06-25T15:36:48.307Z level=INFO source=download.go:177 msg="downloading 5efd52d6d9f2 in 1 487 B part(s)"
[GIN] 2025/06/25 - 15:36:50 | 200 | 32.097565152s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-25T15:36:50.576Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:36:50.678Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:36:50.684Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:36:50.685Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:36:50.685Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="4.9 GiB"
time=2025-06-25T15:36:50.771Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.4 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:36:50.771Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:36:50.771Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="4.9 GiB" memory.required.partial="4.9 GiB" memory.required.kv="1.1 GiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="2.4 GiB" memory.weights.repeating="2.1 GiB" memory.weights.nonrepeating="304.3 MiB" memory.graph.full="768.0 MiB" memory.graph.partial="768.0 MiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 398 tensors from /home/uo289183/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 4B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 4B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2560
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 9728
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  198 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 2.44 GiB (5.20 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 4.02 B
print_info: general.name     = Qwen3 4B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T15:36:50.876Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 2 --port 42499"
time=2025-06-25T15:36:50.876Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:36:50.876Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:36:50.876Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:36:50.882Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:36:50.917Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:36:50.917Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:42499"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 398 tensors from /home/uo289183/.ollama/models/blobs/sha256-163553aea1b1de62de7c5eb2ef5afb756b4b3133308d9ae7e42e951d8d696ef5 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 4B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 4B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2560
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 9728
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  198 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 2.44 GiB (5.20 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2560
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 9728
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 4B
print_info: model params     = 4.02 B
print_info: general.name     = Qwen3 4B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T15:36:51.132Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  2493.69 MiB
load_tensors:   CPU_Mapped model buffer size =   304.28 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.18 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1
init:      CUDA0 KV buffer size =  1152.00 MiB
llama_context: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:      CUDA0 compute buffer size =   554.00 MiB
llama_context:  CUDA_Host compute buffer size =    21.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-06-25T15:36:51.383Z level=INFO source=server.go:628 msg="llama runner started in 0.51 seconds"
[GIN] 2025/06/25 - 15:36:51 | 200 |  813.557533ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:36:58.579Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:37:06 | 200 |  7.650060393s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:37:06.241Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:37:12 | 200 |  6.589887734s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:37:12.833Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:37:18 | 200 |  6.105234448s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:37:18.947Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:37:25 | 200 |   6.21542485s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:37:25.171Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:37:49 | 200 | 24.031056516s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:37:49.214Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:02 | 200 | 12.863145402s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:02.087Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:06 | 200 |  4.675275432s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:06.772Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:11 | 200 |  4.801654015s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:11.582Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:17 | 200 |  5.807699043s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:17.399Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:26 | 200 |  9.520764987s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:26.929Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:34 | 200 |  7.541678948s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:34.480Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:37 | 200 |  2.940995347s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:37.449Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:40 | 200 |  2.760494146s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:40.222Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:44 | 200 |  4.406814482s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:44.642Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:51 | 200 |  6.850508079s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:51.498Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:52 | 200 |  870.375171ms |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:52.377Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:54 | 200 |  1.712060201s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:54.098Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:38:58 | 200 |  4.440312279s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:38:58.547Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:05 | 200 |  7.219675038s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:05.776Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:16 | 200 | 10.283546864s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:16.077Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:19 | 200 |  3.881879933s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:19.963Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:21 | 200 |  1.595493815s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:21.565Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:23 | 200 |  2.371546723s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:23.946Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:31 | 200 |  7.804384477s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:31.761Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:37 | 200 |  5.609703164s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:37.379Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:39 | 200 |  1.888527942s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:39.277Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:43 | 200 |  4.122273063s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:43.409Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:39:51 | 200 |   8.28715737s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:39:51.705Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:02 | 200 | 10.801389119s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:02.515Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:04 | 200 |  1.832849019s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:04.365Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:14 | 200 | 10.570417806s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:14.942Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:25 | 200 | 10.260446869s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:25.210Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:38 | 200 | 13.721219705s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:38.940Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:49 | 200 |  10.62114467s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:40:49.571Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:40:58 | 200 |  9.312039754s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:44:00 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:44:00.568Z level=INFO source=images.go:463 msg="total blobs: 42"
time=2025-06-25T15:44:00.569Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:44:00.570Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:44:00.570Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:44:00.738Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:44:04 | 200 |     502.273µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:44:04 | 200 |     366.595µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:44:05.677Z level=INFO source=download.go:177 msg="downloading a3de86cd1c13 in 16 326 MB part(s)"
time=2025-06-25T15:44:52.950Z level=INFO source=download.go:177 msg="downloading 05a61d37b084 in 1 487 B part(s)"
[GIN] 2025/06/25 - 15:44:56 | 200 | 51.330677533s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-25T15:44:56.338Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:44:56.441Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:44:56.447Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:44:56.448Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:44:56.448Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="7.0 GiB"
time=2025-06-25T15:44:56.538Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.4 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:44:56.538Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:44:56.538Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="7.0 GiB" memory.required.partial="7.0 GiB" memory.required.kv="1.1 GiB" memory.required.allocations="[7.0 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="768.0 MiB" memory.graph.partial="768.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T15:44:56.641Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 8 --parallel 2 --port 38069"
time=2025-06-25T15:44:56.642Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:44:56.642Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:44:56.642Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:44:56.651Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:44:56.687Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:44:56.687Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:38069"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/uo289183/.ollama/models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T15:44:56.893Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.19 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1
init:      CUDA0 KV buffer size =  1152.00 MiB
llama_context: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
llama_context:      CUDA0 compute buffer size =   560.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-06-25T15:44:57.395Z level=INFO source=server.go:628 msg="llama runner started in 0.75 seconds"
[GIN] 2025/06/25 - 15:44:57 | 200 |  1.065003401s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:45:01.744Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:45:13 | 200 | 11.549278589s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:45:13.300Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:45:23 | 200 |  9.873056064s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:45:23.182Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:45:31 | 200 |  8.288298043s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:45:31.480Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:45:40 | 200 |  9.471299961s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:45:40.960Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:04 | 200 | 23.942826798s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:04.920Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:20 | 200 |  16.07384026s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:21.005Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:28 | 200 |  7.975597656s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:28.990Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:35 | 200 |  6.959289289s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:35.959Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:44 | 200 |  8.702852881s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:44.670Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:46:55 | 200 | 11.239538788s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:46:55.919Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:05 | 200 |  9.807997132s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:05.745Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:14 | 200 |  8.658745575s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:14.417Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:17 | 200 |  3.014644981s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:17.447Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:22 | 200 |  5.292323336s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:22.742Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:34 | 200 |  11.45979627s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:34.211Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:35 | 200 |  1.484077477s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:35.704Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:39 | 200 |  4.181458271s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:39.894Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:47 | 200 |  8.015025344s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:47.919Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:47:59 | 200 | 11.261360608s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:47:59.189Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:10 | 200 | 11.480580219s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:10.679Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:19 | 200 |   8.80475256s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:19.493Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:22 | 200 |  3.196004164s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:22.700Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:27 | 200 |  4.570630597s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:27.278Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:36 | 200 |  9.281984912s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:36.569Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:44 | 200 |  7.694154226s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:44.273Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:47 | 200 |  3.113456946s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:47.395Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:48:53 | 200 |  5.644051179s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:48:53.048Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:49:11 | 200 | 18.462591691s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:49:11.526Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:49:24 | 200 | 13.091657311s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:49:24.626Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:49:27 | 200 |  3.242513074s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:49:27.877Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:49:34 | 200 |  6.255249018s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:49:34.141Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:49:52 | 200 | 18.390658098s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:49:52.553Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:50:18 | 200 | 25.739340892s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:50:18.347Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:50:34 | 200 | 16.352870107s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:50:34.711Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:50:45 | 200 | 10.622203099s |       127.0.0.1 | POST     "/api/generate"
2025/06/25 15:54:56 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-25T15:54:56.311Z level=INFO source=images.go:463 msg="total blobs: 44"
time=2025-06-25T15:54:56.312Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-25T15:54:56.312Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-25T15:54:56.313Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-25T15:54:56.473Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/25 - 15:55:00 | 200 |    1.153262ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/25 - 15:55:00 | 200 |     716.344µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-25T15:55:01.749Z level=INFO source=download.go:177 msg="downloading a8cc1361f314 in 16 579 MB part(s)"
time=2025-06-25T15:56:53.109Z level=INFO source=download.go:177 msg="downloading 78b3b822087d in 1 488 B part(s)"
[GIN] 2025/06/25 - 15:56:58 | 200 |         1m57s |       127.0.0.1 | POST     "/api/pull"
time=2025-06-25T15:56:58.250Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:56:58.348Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:56:58.354Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-25T15:56:58.355Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:56:58.355Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="11.2 GiB"
time=2025-06-25T15:56:58.439Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="120.4 GiB" free_swap="8.0 GiB"
time=2025-06-25T15:56:58.439Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-25T15:56:58.440Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-25T15:56:58.546Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 35365"
time=2025-06-25T15:56:58.546Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-25T15:56:58.546Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-25T15:56:58.547Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-25T15:56:58.553Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-25T15:56:58.588Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-25T15:56:58.589Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:35365"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-25T15:56:58.797Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-25T15:56:59.801Z level=INFO source=server.go:628 msg="llama runner started in 1.25 seconds"
[GIN] 2025/06/25 - 15:56:59 | 200 |  1.558534835s |       127.0.0.1 | POST     "/api/chat"
time=2025-06-25T15:57:50.552Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:58:11 | 200 | 21.221410197s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:58:11.786Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:58:26 | 200 | 14.245359904s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:58:26.041Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:58:37 | 200 |  11.53292438s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:58:37.584Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:58:50 | 200 | 13.416419841s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:58:51.009Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:59:22 | 200 | 31.836428546s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:59:22.859Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:59:41 | 200 | 18.230614629s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:59:41.101Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 15:59:57 | 200 | 16.235389443s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T15:59:57.349Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:00:07 | 200 | 10.240618245s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:00:07.598Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:00:22 | 200 | 15.119844571s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:00:22.727Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:00:41 | 200 | 19.273949523s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:00:42.016Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:00:53 | 200 | 11.427027853s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:00:53.454Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:00 | 200 |  6.909053985s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:00.377Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:07 | 200 |  6.952388597s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:07.339Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:15 | 200 |   7.92982722s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:15.271Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:31 | 200 | 16.147840205s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:31.431Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:34 | 200 |  2.927652724s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:34.368Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:41 | 200 |    7.1927053s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:41.569Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:01:56 | 200 | 15.136496112s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:01:56.715Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:02:13 | 200 | 16.704853734s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:02:13.432Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:02:30 | 200 | 16.590910755s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:02:30.039Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:02:41 | 200 | 11.761601818s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:02:41.809Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:02:50 | 200 |  8.527049497s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:02:50.346Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:02:57 | 200 |  7.529024995s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:02:57.888Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:03:19 | 200 | 21.405161197s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:03:19.305Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:03:36 | 200 | 16.868725188s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:03:36.185Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:03:41 | 200 |  5.336392349s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:03:41.531Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:03:48 | 200 |  7.285719277s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:03:48.826Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:04:18 | 200 | 29.949691095s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:04:18.788Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:04:41 | 200 | 22.780852546s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:04:41.582Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:04:44 | 200 |  2.882649227s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:04:44.474Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:04:56 | 200 | 12.332229137s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:04:56.815Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:05:24 | 200 | 27.655163593s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:05:24.494Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:05:49 | 200 | 25.118675639s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:05:49.621Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:06:20 | 200 | 30.584462299s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-25T16:06:20.220Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/25 - 16:06:41 | 200 | 21.599624768s |       127.0.0.1 | POST     "/api/generate"
2025/06/27 15:27:26 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-27T15:27:26.401Z level=INFO source=images.go:463 msg="total blobs: 46"
time=2025-06-27T15:27:26.402Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-27T15:27:26.402Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-27T15:27:26.402Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-27T15:27:26.569Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/27 - 15:27:36 | 200 |     573.841µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:27:36 | 200 |     498.792µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:27:36.814Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:27:36.913Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:27:36.920Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:27:36.921Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:27:36.921Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="11.2 GiB"
time=2025-06-27T15:27:37.007Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.2 GiB" free_swap="8.0 GiB"
time=2025-06-27T15:27:37.007Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:27:37.007Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T15:27:37.113Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 32791"
time=2025-06-27T15:27:37.113Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T15:27:37.113Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T15:27:37.113Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T15:27:37.121Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T15:27:37.157Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T15:27:37.157Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:32791"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T15:27:37.545Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T15:27:38.299Z level=INFO source=server.go:628 msg="llama runner started in 1.19 seconds"
[GIN] 2025/06/27 - 15:27:38 | 200 |  1.491585388s |       127.0.0.1 | POST     "/api/chat"
2025/06/27 15:28:18 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-27T15:28:18.805Z level=INFO source=images.go:463 msg="total blobs: 46"
time=2025-06-27T15:28:18.806Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-27T15:28:18.807Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-27T15:28:18.807Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-27T15:28:18.972Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/27 - 15:28:23 | 200 |     604.832µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:28:23 | 200 |    1.006142ms |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:28:23.953Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:28:24.061Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:28:24.069Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:28:24.070Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:28:24.070Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=16462118912 required="11.2 GiB"
time=2025-06-27T15:28:24.158Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="119.3 GiB" free_swap="8.0 GiB"
time=2025-06-27T15:28:24.158Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:28:24.158Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[15.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T15:28:24.263Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 45141"
time=2025-06-27T15:28:24.263Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T15:28:24.263Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T15:28:24.263Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T15:28:24.269Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T15:28:24.304Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T15:28:24.304Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:45141"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 15699 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T15:28:24.514Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T15:28:25.517Z level=INFO source=server.go:628 msg="llama runner started in 1.25 seconds"
[GIN] 2025/06/27 - 15:28:25 | 200 |  1.571408998s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 15:28:57 | 200 |     493.315µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:28:57 | 200 |    1.009653ms |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:28:57.341Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 15:28:57 | 200 |     6.94472ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 15:29:20 | 200 |      573.61µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:29:20 | 200 |     444.412µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:29:20.371Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 15:29:20 | 200 |    7.559479ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 15:52:22 | 200 |     528.032µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:52:22 | 200 |     433.622µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:52:22.971Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:52:23.068Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:52:23.075Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:52:23.075Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:52:23.075Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15651241984 required="11.2 GiB"
time=2025-06-27T15:52:23.165Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="118.3 GiB" free_swap="8.0 GiB"
time=2025-06-27T15:52:23.165Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:52:23.165Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[14.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T15:52:23.268Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 44877"
time=2025-06-27T15:52:23.269Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T15:52:23.269Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T15:52:23.269Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T15:52:23.275Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T15:52:23.310Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T15:52:23.311Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:44877"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14926 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T15:52:23.670Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T15:52:24.423Z level=INFO source=server.go:628 msg="llama runner started in 1.15 seconds"
[GIN] 2025/06/27 - 15:52:24 | 200 |  1.459163271s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 15:52:41 | 200 |    1.133079ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:52:41 | 200 |     465.218µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:52:41.160Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 15:52:41 | 200 |    7.241199ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 15:58:30 | 200 |     568.212µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 15:58:30 | 200 |     462.927µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T15:58:30.334Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:58:30.432Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:58:30.439Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T15:58:30.439Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:58:30.439Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15651241984 required="11.2 GiB"
time=2025-06-27T15:58:30.526Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="117.5 GiB" free_swap="8.0 GiB"
time=2025-06-27T15:58:30.526Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T15:58:30.526Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[14.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T15:58:30.631Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 42305"
time=2025-06-27T15:58:30.631Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T15:58:30.631Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T15:58:30.631Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T15:58:30.638Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T15:58:30.674Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T15:58:30.674Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:42305"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14926 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T15:58:30.882Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T15:58:31.886Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/27 - 15:58:31 | 200 |  1.559833543s |       127.0.0.1 | POST     "/api/chat"
2025/06/27 16:27:47 routes.go:1233: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:49152 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/uo289183/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-06-27T16:27:47.258Z level=INFO source=images.go:463 msg="total blobs: 46"
time=2025-06-27T16:27:47.259Z level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-06-27T16:27:47.259Z level=INFO source=routes.go:1300 msg="Listening on 127.0.0.1:49152 (version 0.6.8)"
time=2025-06-27T16:27:47.259Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-06-27T16:27:47.413Z level=INFO source=types.go:130 msg="inference compute" id=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e library=cuda variant=v12 compute=8.9 driver=12.8 name="NVIDIA GeForce RTX 4080 SUPER" total="15.6 GiB" available="15.3 GiB"
[GIN] 2025/06/27 - 16:28:39 | 200 |    1.290329ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:28:39 | 200 |     574.134µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:28:39.701Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:28:39.797Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:28:39.803Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:28:39.804Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T16:28:39.804Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15651241984 required="11.2 GiB"
time=2025-06-27T16:28:39.894Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="118.0 GiB" free_swap="8.0 GiB"
time=2025-06-27T16:28:39.894Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T16:28:39.895Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[14.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T16:28:39.998Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 33719"
time=2025-06-27T16:28:39.998Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T16:28:39.998Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T16:28:39.999Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T16:28:40.008Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T16:28:40.043Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T16:28:40.043Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:33719"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14926 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T16:28:40.250Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T16:28:41.254Z level=INFO source=server.go:628 msg="llama runner started in 1.26 seconds"
[GIN] 2025/06/27 - 16:28:41 | 200 |  1.559045598s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:33:21 | 200 |     527.527µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:33:21 | 200 |     428.684µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:33:21.291Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:33:21 | 200 |    9.875357ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:37:33 | 200 |     505.768µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:37:33 | 200 |     435.262µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:37:33.366Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:37:33 | 200 |    7.704853ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:38:02.752Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:38:14 | 200 | 11.676429818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 16:39:55 | 200 |     506.478µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:39:55 | 200 |     455.703µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:39:55.518Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:39:55 | 200 |    7.353124ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:40:11.843Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:40:30 | 200 | 19.018620709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 16:41:55 | 200 |    1.138444ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:41:55 | 200 |     386.614µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:41:55.462Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:41:55 | 200 |    7.357165ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:42:07.828Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:42:13 | 200 |  5.488030473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 16:43:57 | 200 |     518.583µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:43:57 | 200 |     430.646µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:43:57.093Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:43:57 | 200 |    14.23574ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:44:10 | 200 |     585.858µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:44:10 | 200 |     421.838µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:44:10.591Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:44:10 | 200 |    7.781524ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:44:54 | 200 |     576.675µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:44:54 | 200 |     439.489µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:44:54.644Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:44:54 | 200 |    7.671238ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:45:09.083Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:45:22 | 200 | 13.101007813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 16:46:45 | 200 |     514.612µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:46:45 | 200 |     965.976µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:46:45.978Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:46:45 | 200 |    7.254897ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:47:03.217Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:47:11 | 200 |  8.626518484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/06/27 - 16:53:48 | 200 |     714.987µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:53:48 | 200 |     427.439µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:53:48.150Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:53:48.249Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:53:48.255Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
time=2025-06-27T16:53:48.255Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T16:53:48.256Z level=INFO source=sched.go:754 msg="new model will fit in available VRAM in single GPU, loading" model=/home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e gpu=GPU-4ef17693-6c06-0b1c-153c-65c701cc2f4e parallel=2 available=15651241984 required="11.2 GiB"
time=2025-06-27T16:53:48.343Z level=INFO source=server.go:106 msg="system memory" total="125.6 GiB" free="118.0 GiB" free_swap="8.0 GiB"
time=2025-06-27T16:53:48.343Z level=WARN source=ggml.go:152 msg="key not found" key=qwen3.vision.block_count default=0
time=2025-06-27T16:53:48.344Z level=INFO source=server.go:139 msg=offload library=cuda layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[14.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="11.2 GiB" memory.required.partial="11.2 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[11.2 GiB]" memory.weights.total="8.2 GiB" memory.weights.repeating="7.6 GiB" memory.weights.nonrepeating="608.6 MiB" memory.graph.full="1.0 GiB" memory.graph.partial="1.0 GiB"
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-06-27T16:53:48.447Z level=INFO source=server.go:410 msg="starting llama server" cmd="/home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/bin/ollama runner --model /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e --ctx-size 8192 --batch-size 512 --n-gpu-layers 41 --threads 8 --parallel 2 --port 39535"
time=2025-06-27T16:53:48.448Z level=INFO source=sched.go:452 msg="loaded runners" count=1
time=2025-06-27T16:53:48.448Z level=INFO source=server.go:589 msg="waiting for llama runner to start responding"
time=2025-06-27T16:53:48.448Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server not responding"
time=2025-06-27T16:53:48.454Z level=INFO source=runner.go:853 msg="starting go runner"
load_backend: loaded CPU backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/libggml-cpu-alderlake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4080 SUPER, compute capability 8.9, VMM: yes
load_backend: loaded CUDA backend from /home/uo289183/TFG/Multimodal-IA--TFG/server/app/third-party/ollama/bin/ollama/lib/ollama/cuda_v12/libggml-cuda.so
time=2025-06-27T16:53:48.491Z level=INFO source=ggml.go:103 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-06-27T16:53:48.491Z level=INFO source=runner.go:913 msg="Server listening on 127.0.0.1:39535"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4080 SUPER) - 14926 MiB free
llama_model_loader: loaded meta data with 27 key-value pairs and 443 tensors from /home/uo289183/.ollama/models/blobs/sha256-a8cc1361f3145dc01f6d77c6c82c9116b9ffe3c97b34716fe20418455876c40e (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 14B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 14B
llama_model_loader: - kv   5:                          qwen3.block_count u32              = 40
llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 17408
llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - kv  26:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  161 tensors
llama_model_loader: - type  f16:   40 tensors
llama_model_loader: - type q4_K:  221 tensors
llama_model_loader: - type q6_K:   21 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 8.63 GiB (5.02 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 5120
print_info: n_layer          = 40
print_info: n_head           = 40
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 5
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 17408
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 14B
print_info: model params     = 14.77 B
print_info: general.name     = Qwen3 14B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-06-27T16:53:48.699Z level=INFO source=server.go:623 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:        CUDA0 model buffer size =  8423.47 MiB
load_tensors:   CPU_Mapped model buffer size =   417.30 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     1.20 MiB
init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1
init:      CUDA0 KV buffer size =  1280.00 MiB
llama_context: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   696.00 MiB
llama_context:  CUDA_Host compute buffer size =    26.01 MiB
llama_context: graph nodes  = 1526
llama_context: graph splits = 2
time=2025-06-27T16:53:49.702Z level=INFO source=server.go:628 msg="llama runner started in 1.25 seconds"
[GIN] 2025/06/27 - 16:53:49 | 200 |  1.558853164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:54:33 | 200 |       557.5µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:54:33 | 200 |     900.156µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:54:33.792Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:54:33 | 200 |    7.242759ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/06/27 - 16:55:57 | 200 |     499.789µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/06/27 - 16:55:57 | 200 |     480.347µs |       127.0.0.1 | GET      "/api/tags"
time=2025-06-27T16:55:57.491Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:55:57 | 200 |    7.908575ms |       127.0.0.1 | POST     "/api/chat"
time=2025-06-27T16:56:05.837Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:56:13 | 200 |  7.841147805s |       127.0.0.1 | POST     "/api/generate"
time=2025-06-27T16:56:40.765Z level=WARN source=ggml.go:152 msg="key not found" key=general.alignment default=32
[GIN] 2025/06/27 - 16:56:46 | 200 |  5.293545877s |       127.0.0.1 | POST     "/api/generate"
