

# ---- M√ìDULOS ---- #
from ollama.client import OllamaClient
from ollama.response import Response

from config.context import CONFIG

# ---- CLASES ---- #
class ChatSession:
    """
    Clase que gestiona una sesi√≥n de chat simple con un modelo LLM local a trav√©s 
    de Ollama.
    """
    # -- M√©todos por defecto -- #
    def __init__(self):
        """
        Inicializa la sesi√≥n de chat con un modelo Ollama dado.
        """
        self.__client:OllamaClient = OllamaClient()  # Inicializa el cliente de Ollama.
        self.__exitCommands:list[str] = ["EXIT", "QUIT", "SALIR"]
    
    # -- M√©todos p√∫blicos -- #
    def Start(self) -> None:
        """
        Inicia el bucle de chat. Solicita entradas al usuario y muestra respuestas del modelo.
        Finaliza cuando el usuario escribe una palabra de salida.
        """
        # B√∫cle infinito para el chat.
        while True:
            prompt = input("üß† Usuario: ")          # Obtiene el mensaje del usuario.
            # Si el mensaje del usuario es un mensaje de salida.
            if prompt.upper().strip() in CONFIG.chat.exitCommands:
                break                       # Finaliza el b√∫cle.
            
            # Env√≠a y recibe el mensaje del modelo.
            reply:Response = self.__client.send_message(message=prompt)
            # Si se ha recibido una respuesta.
            if reply:
                print(f"ü§ñ {reply.model}: {reply.response}")                    # Imprime la respuesta.
            # Si no se ha recibido una respuesta.
            else:
                print(f"‚ö†Ô∏è Advertencia: no se pudo recibir ning√∫n mensaje.")    # Imprime el mensaje.