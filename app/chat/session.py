# -----------------------------------------------------------------------------
# MULTIMODAL-IA--TFG - Proyecto TFG
# (c) 2025 Pablo Gonz√°lez Garc√≠a
# Universidad de Oviedo, Escuela Polit√©cncia de Ingenier√≠a de Gij√≥n
# Archivo: app/utils/log/logger.py
# Autor: Pablo Gonz√°lez Garc√≠a
# Descripci√≥n:
# M√≥dulo con clases y funciones encargadas de gestionar la sesi√≥n
# del cliente con el modelo de Ollama.
# -----------------------------------------------------------------------------


# ---- M√ìDULOS ---- #
from ollama.client import OllamaClient
from ollama.response import Response

from prompt.base import PromptStrategy
from prompt.jinja import JinjaPrompt

from ollama.chat import ChatHistory

from config.context import CONFIG

# ---- CLASES ---- #
class ChatSession:
    """
    Clase que gestiona una sesi√≥n de chat simple con un modelo LLM local a trav√©s 
    de Ollama.
    """
    # -- M√©todos por defecto -- #
    def __init__(self):
        """
        Inicializa la sesi√≥n de chat con un modelo Ollama dado.
        """
        self.__client:OllamaClient = OllamaClient()  # Inicializa el cliente de Ollama.
        self.__chatHistory:ChatHistory = ChatHistory(CONFIG.ollama.historyMaxSize)                  # Inicializa el historial de mensajes.
        self.__promtpStrategy:PromptStrategy = JinjaPrompt()    # Inicializa la estrategia de prompting.
    
    # -- M√©todos p√∫blicos -- #
    def Start(self) -> None:
        """
        Inicia el bucle de chat. Solicita entradas al usuario y muestra respuestas del modelo.
        Finaliza cuando el usuario escribe una palabra de salida.
        """
        # B√∫cle infinito para el chat.
        while True:
            user_input = input("üß† Usuario: ")          # Obtiene el mensaje del usuario.
            # Si el mensaje del usuario es un mensaje de salida.
            if user_input.upper().strip() in CONFIG.chat.exitCommands:
                break                       # Finaliza el b√∫cle.
            
            # A√±ade el mensaje al historial.
            self.__chatHistory.add_message(role="user", message=user_input)
            # Genera el prompt.
            prompt:str = self.__promtpStrategy.build_prompt(user_input=user_input, history=self.__chatHistory.Messages)
            # Env√≠a y recibe el mensaje del modelo.
            reply:Response = self.__client.send_message(prompt=prompt)
            # Si se ha recibido una respuesta.
            if reply:
                print(f"ü§ñ {reply.model}: {reply.response}")                    # Imprime la respuesta.
                # A√±ade el mensaje al historial.
                self.__chatHistory.add_message(role="assistant", message=reply.response)
            # Si no se ha recibido una respuesta.
            else:
                print(f"‚ö†Ô∏è Advertencia: no se pudo recibir ning√∫n mensaje.")    # Imprime el mensaje.