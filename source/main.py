# ---- Modulos ---- #
from config.settings import MODEL_CACHE_PATH
from ia.model import LocalLLM

# ---- FunciÃ³n principal ---- #
if __name__ == "__main__":
    
    # Imprime informaciÃ³n de ejecuciÃ³n:
    print("Running program ...")
    
    localLLM = LocalLLM(modelId="deepseek-ai/DeepSeek-R1-Distill-Llama-8B")
    
    print("ðŸ§  Pregunta: Â¿QuiÃ©n eres?")
    response = localLLM.MakeQuestion("Â¿QuiÃ©n eres?")
    print("ðŸ¤– Respuesta: ", response)