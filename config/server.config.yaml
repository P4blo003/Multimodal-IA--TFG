# -----------------------------------------------------------------------------
# MULTIMODAL-IA--TFG - Proyecto TFG
# (c) 2025 Pablo González García
# Universidad de Oviedo, Escuela Politécncia de Ingeniería de Gijón
# Archivo: config/server.config.yaml
# Autor: Pablo González García
# Descripción:
# Archivo de configuración principal del asistente inteligente.
# Contiene parámetros relacionados con el sistema de logging,
# la conexión a bases de datos, y otros ajustes generales del sistema.
# -----------------------------------------------------------------------------


# ---- CONFIGURACIÓN ---- #
# -- Configuración de uvicorn -- #
uvicorn:
  host: 'localhost'           # Host de uvicorn.
  port: 9000                  # Puerto de uvicorn.

# -- Configuración del logger de la aplicación -- #
logger:
  level: INFO                 # Nivel de log: DEBIG, INFO, WARNING, ERROR, CRITICAL
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'    # Formato del mensaje del log.
  datefmt: '%d-%m-%Y %H:%M:%S'    # Formato de la fecha.
  maxBytes: 10485760          # 10 MB.
  backupCount: 5              # Número de archivos de log de respaldo.

# -- Configuración de ollama -- #
ollama:
  exe: 'bin/ollama/bin/ollama'  # Ruta del binario.
  host: 'localhost'             # Host del servicio ollama.
  port: 9999                    # Puerto del servicio ollama.
  silent: False                 # Si se debe mostrar los mensajes del propio servicio de ollama.
  startupWaitSeconds: 5         # Tiempo de espera para el inicio del servicio (5 segundos).
  logFile: 'ollama.serve.log'   # Nombre del fichero de logs de ollama.

# -- Configuración del modelo -- #
model:
  name: 'llama3.1:8b'             # Nombre del modelo a utilizar.
  temperature: 0.0                # Parámetro que controla la aleatoriedad de las respuestas.

# -- Configuración del chat -- #
chat:
  maxHistorySize: 20              # Máximo tamaño del historial del chat.

# -- Configuración del RAG -- #
rag:
  # Configuración general del RAG.
  backend: 'LANGCHAIN'                   # El backend empleado. Puede ser HAYSTACK | LANGCHAIN
  docDirectory: 'data/raw'              # Directorio de documentos.
  persistDirectory: 'data/persistence'  # Directorio de persisntencia de los embeddings calculados.
  splitLength: 500                      # Tamaño del chunk en tokens/palabras. (En LangChain es similar a chunk_size).
  splitOverlap: 50                      # Overlap para mantener contexto entre chunks. (En LangChain es similar a chunk_overlap).
  topK: 10                              # El número de chunks más relevantes que se recuperan.
  embeddingDim: 1024                                # Tamaño del embedding.
  # Configuración especifica del embedding.
  embedding:
    model: 'sentence-transformers/all-mpnet-base-v2'   # Modelo de embedding.
    file: 'data/cache/embedding.models.json'          # Fichero que registra la ubicación de los modelos de embedding.
    persistDirectory: 'resources/models'              # Directorio de almacenamiento de modelos.

# -- Configuración del prompt -- #
prompt:
  templateFile: 'config/prompt.template.txt'                # Fichero con el template del prompt.
  variables: ['context', 'history', 'user_input']           # Variables que espera el prompt.