# -----------------------------------------------------------------------------
# MULTIMODAL-IA--TFG - Proyecto TFG
# (c) 2025 Pablo González García
# Universidad de Oviedo, Escuela Politécncia de Ingeniería de Gijón
# Archivo: config/server_config.yaml
# Autor: Pablo González García
# Descripción:
# Archivo de configuración principal del asistente inteligente.
# Contiene parámetros relacionados con el sistema de logging,
# la conexión a bases de datos, y otros ajustes generales del sistema.
# -----------------------------------------------------------------------------


# ---- CONFIGURACIÓN ---- #
# -- Configuración del sistema -- #
system:
  logDir: 'logs'    # Directorio de logs.

# -- Configuración del logger de la aplicación -- #
logger:
  level: INFO                 # Nivel de log: DEBIG, INFO, WARNING, ERROR, CRITICAL
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'    # Formato del mensaje del log.
  datefmt: '%d-%m-%Y %H:%M:%S'    # Formato de la fecha.
  maxBytes: 10485760          # 10 MB.
  backupCount: 5              # Número de archivos de log de respaldo.

# -- Configuración de uvicorn -- #
uvicorn:
  host: 'localhost' # Host del servicio de uvicorn.
  port: 9999        # Puerto del servicio de uvicorn.
  reload: False     # Si se desea que se haga reload al modificarse algún script.
  logLevel: 'INFO'  # Nivel mínimo del log de uvicorn.

# -- Configuración de Ollama -- #
ollama:
  exe: 'bin/ollama/bin/ollama'  # Fichero binario de Ollama.
  api:
    host: '127.0.0.1' # Host de la API de Ollama.
    port: 9000        # Puerto de la API de Ollama.
    silent: False     # Si la salida de la API debe ser silenciosa.
    startupWaitSeconds: 5     # Tiempo de espera hasta que se inicie la API de Ollama.
    logFile: 'ollama_api.log' # Fichero de logs de la API de Ollama.
    model:
      name: 'llama3.1:8b'             # Nombre del modelo a usar por la API de Ollama
      temperature: 0.0                # Parámetro que controla la aleatoriedad de las respuestas.
  
# -- Configuración del chat -- #
chat:
  maxHistorySize: 20  # Tamaño máximo del historial del chat.

# -- Configuración del RAG -- #
rag:
  backend: 'HAYSTACK'               # El backend empleado.
  dataDir: 'data/raw'               # Directorio de donde obtener los datos.
  persistDir: 'data/persistence'    # Directorio de persistencia de los embeddings.
  storeName: 'storage'      # Nombre para los almacenes.
  splitLength: 500          # Tamaño del chunk en tokens.
  splitOverlap: 50          # Overlap para mantener contexto entre chunks.
  topK: 10                  # El número de chunks más relvantes.
  embeddingDim: 768         # Tamaño del embedding.
  embedder:
    name: 'sentence-transformers/all-mpnet-base-v2'     # Nombre del modelo de embedding.
    persistDir: 'resources/models'                      # Directorio de almacenamiento de modelos.

# -- Configuración del prompt -- #
prompt:
  templateFile: 'config/prompt_template.j2'                 # Fichero con el template del prompt.
  variables: ['context', 'history', 'user_input']           # Variables que espera el prompt.